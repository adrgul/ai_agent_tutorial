# P0.8 ‚Äì Intelligent RAG Routing (Phase 2.0 Enhancement)

## üìã ST√ÅTUSZ
- **F√°zis:** P0.8 (P0.7 tov√°bbfejleszt√©se)
- **T√≠pus:** Enhancement / UX Improvement
- **Implement√°ci√≥ d√°tuma:** 2025-12-27
- **√Ållapot:** ‚úÖ COMPLETED

---

## üéØ PROBL√âMA

**P0.7 eredeti workflow:**
```
START ‚Üí validate_input ‚Üí build_context ‚Üí retrieve_document_chunks ‚Üí check_retrieval_results ‚Üí [YES/NO] ‚Üí END
```

**Probl√©ma:**
- **MINDEN query** Qdrant document retrieval-t ind√≠tott
- Egyszer≈± k√∂sz√∂n√©sek ("szia", "hello") is dokumentum keres√©st triggereltek
- Konverz√°ci√≥s query-k ("hogy vagy?", "mi a helyzet?") is RAG pipeline-on mentek √°t
- Rossz UX: fallback v√°lasz m√©g akkor is, ha a user csak besz√©lget

**P√©lda probl√©ma:**
```
User: "szia"
Assistant: "Sajnos nem tal√°ltam relev√°ns dokumentumot a k√©rd√©sedhez..."
```

Ez **nem** term√©szetes chat √©lm√©ny.

---

## üí° MEGOLD√ÅS

**√öj node besz√∫r√°sa:** `decide_if_rag_needed`

**√öjonnan m√≥dos√≠tott workflow:**
```
START
 ‚Üí validate_input          [tenant_id, user_id, query check]
 ‚Üí build_context           [prompt hierarchia]
 ‚Üí decide_if_rag_needed    [üÜï LLM decision: kell-e dokumentum?]
    ‚îú‚îÄ YES (needs_rag) ‚Üí retrieve_document_chunks ‚Üí check_retrieval_results ‚Üí [generate_answer | fallback]
    ‚îî‚îÄ NO  (no_rag)    ‚Üí generate_direct_answer [norm√°l chat, NINCS RAG]
 ‚Üí END
```

**Kulcs k√ºl√∂nbs√©g:**
- **Nem minden query megy RAG-be**
- LLM intelligensen d√∂nt: conversational vs. document-retrieval query
- Gyorsabb v√°lasz egyszer≈± query-kn√©l (nincs embedding + Qdrant overhead)

---

## üß† LLM DECISION LOGIC

### Node: `decide_if_rag_needed`

**Input:**
- `query`: felhaszn√°l√≥ k√©rd√©se
- `user_language`: hu | en

**Prompt strat√©gia (magyar):**
```
D√∂ntsd el, hogy a felhaszn√°l√≥ k√©rd√©se ig√©nyel-e dokumentum keres√©st √©s lek√©r√©st (RAG).

K√âRD√âS: "{query}"

V√°laszolj CSAK egy sz√≥val:
- "IGEN" - ha a k√©rd√©s konkr√©t inform√°ci√≥t, tartalmat, vagy dokumentumra utal
- "NEM" - ha k√∂sz√∂n√©s, √°ltal√°nos besz√©lget√©s, vagy k√∂z√∂ns√©ges k√©rd√©s

V√°lasz (CSAK IGEN vagy NEM):
```

**D√∂nt√©s parsing:**
```python
needs_rag = decision_text in ["YES", "IGEN", "Y", "I"]
```

**Output:**
- `needs_rag: bool` ‚Üí routing decision

### Routing: `_route_based_on_rag_decision`

```python
def _route_based_on_rag_decision(self, state: RAGState) -> str:
    needs_rag = state.get("needs_rag", True)
    return "needs_rag" if needs_rag else "no_rag"
```

**Conditional edges:**
- `needs_rag` ‚Üí `retrieve_document_chunks` (eredeti RAG path)
- `no_rag` ‚Üí `generate_direct_answer` (√∫j conversational path)

---

## üìä WORKFLOW PATHS

### Path A: Conversational (no RAG)
```
validate_input ‚Üí build_context ‚Üí decide_if_rag_needed [NO] ‚Üí generate_direct_answer ‚Üí END
```

**P√©lda query-k:**
- "szia" / "hello"
- "hogy vagy?" / "how are you?"
- "mi a helyzet?" / "what's up?"
- "k√∂sz√∂n√∂m" / "thank you"

**V√°lasz forr√°sa:**
- LLM (GPT-3.5-turbo)
- System prompt (hierarchical: system.ini + tenant + user)
- **NINCS** dokumentum context
- **sources: []** (√ºres forr√°s lista)

### Path B: RAG (document retrieval)
```
validate_input ‚Üí build_context ‚Üí decide_if_rag_needed [YES] ‚Üí retrieve_document_chunks 
  ‚Üí check_retrieval_results ‚Üí [generate_answer_from_context | generate_fallback_response] ‚Üí END
```

**P√©lda query-k:**
- "mi van a dokumentumban?"
- "√∂sszefoglald a jelent√©st"
- "mit √≠r az √∫tmutat√≥ban?"
- "milyen term√©k van?"

**V√°lasz forr√°sa:**
- LLM + Retrieved document chunks
- **sources: [doc_id_1, doc_id_2, ...]** (forr√°s attrib√∫ci√≥)

---

## üîß TECHNIKAI IMPLEMENT√ÅCI√ì

### State m√≥dos√≠t√°s

**√öj field a RAGState-ben:**
```python
class RAGState(TypedDict):
    # Input
    query: str
    user_context: UserContext
    
    # Intermediate
    system_prompt: Optional[str]
    combined_prompt: Optional[str]
    needs_rag: bool  # üÜï LLM decision
    retrieved_chunks: List[DocumentChunk]
    has_relevant_context: bool
    
    # Output
    final_answer: Optional[str]
    sources: List[int]
    error: Optional[str]
```

### √öj node: `_decide_rag_needed_node`

**F√°jl:** `backend/services/rag_workflow.py`

**Funkci√≥:**
1. Query + user_language ‚Üí LLM prompt
2. LLM invoke (ChatOpenAI)
3. Decision text parsing
4. State update: `needs_rag: bool`

**Error handling:**
- LLM call fail ‚Üí default `needs_rag = True` (konzervat√≠v fallback)
- √çgy biztos√≠tjuk, hogy hiba eset√©n sem vesznek el dokumentum-alap√∫ query-k

### √öj node: `_generate_direct_answer_node`

**Funkci√≥:**
- Conversational query-kre v√°laszol (NINCS RAG)
- System prompt + query ‚Üí LLM
- Gyors v√°lasz (nincs embedding, nincs Qdrant)

**Output:**
```python
{
    "final_answer": "Szia! Miben seg√≠thetek?",
    "sources": []  # √úres, mert nincs dokumentum
}
```

### Graph m√≥dos√≠t√°s

**√öj edges:**
```python
workflow.add_edge("build_context", "decide_if_rag_needed")

workflow.add_conditional_edges(
    "decide_if_rag_needed",
    self._route_based_on_rag_decision,
    {
        "needs_rag": "retrieve_document_chunks",
        "no_rag": "generate_direct_answer"
    }
)

workflow.add_edge("generate_direct_answer", END)
```

---

## üìà EL≈êNY√ñK

1. **Jobb UX:**
   - Term√©szetes besz√©lget√©s lehets√©ges
   - K√∂sz√∂n√©sekre emberi v√°lasz
   - Nincs zavar√≥ "nem tal√°ltam dokumentumot" √ºzenet

2. **Gyorsabb v√°lasz:**
   - Conversational query-k: ~1-2s (NINCS embedding + Qdrant)
   - RAG query-k: ~3-5s (teljes pipeline)

3. **K√∂lts√©gmegtakar√≠t√°s:**
   - Kevesebb embedding API h√≠v√°s (csak RAG query-kn√©l)
   - Kevesebb Qdrant search (csak ha sz√ºks√©ges)

4. **Intelligensebb routing:**
   - LLM-based decision (nem keyword matching)
   - Context-aware (user_language figyelembe v√©ve)
   - K√∂nnyen finomhangolhat√≥ prompt m√≥dos√≠t√°ssal

---

## ‚ö†Ô∏è ELT√âR√âSEK AZ EREDETI SPEC-T≈êL

### INIT_PROMPT_2.md P0.7 szerint:

> "NINCS LLM-BASED INTENT DETECTION (t√∫lkomplik√°lt Phase 2.0-ra)."

**Ez P1.3-ra vonatkozott** (document vs product routing), **NEM** P0.7-re.

### P0.8 Enhancement:

- **NEM** document vs product routing (az P1.3)
- **Conversational vs RAG routing** (√∫j use case)
- **NEM** t√∫lkomplik√°lt: egyetlen LLM call, egyszer≈± YES/NO d√∂nt√©s
- **Sz√ºks√©ges UX jav√≠t√°s** a norm√°l chat m≈±k√∂d√©shez

### Specifik√°ci√≥ friss√≠t√©s:

A P0.7 eredeti c√©lja: **RAG workflow m≈±k√∂d√©se dokumentum retrieval-lel**
A P0.8 kieg√©sz√≠t√©se: **+ Intelligens d√∂nt√©s, hogy MIKOR kell RAG**

Ez **nem √ºtk√∂zik** a P1.3 specifik√°ci√≥val, mivel:
- P1.3: document collection vs product collection routing
- P0.8: conversational vs document-retrieval routing

K√©t k√ºl√∂n d√∂nt√©si szint.

---

## ‚úÖ DONE WHEN CHECKLIST

- [x] `RAGState` b≈ëv√≠tve `needs_rag: bool` field-del
- [x] `_decide_rag_needed_node` implement√°lva
- [x] `_route_based_on_rag_decision` conditional router
- [x] `_generate_direct_answer_node` implement√°lva (no RAG path)
- [x] LangGraph edges m√≥dos√≠tva √∫j routing-gal
- [x] Error handling: LLM fail ‚Üí default `needs_rag = True`
- [x] Backend build + deploy sikeres
- [x] Teszt: "szia" ‚Üí conversational v√°lasz (NINCS "nem tal√°ltam dokumentumot")
- [x] Teszt: "mi van a dokumentumban?" ‚Üí RAG pipeline aktiv√°l√≥dik
- [x] Frontend: m≈±k√∂dik mindk√©t path (sources √ºres vs kit√∂lt√∂tt)

---

## üß™ TESZTEL√âSI FORGAT√ìK√ñNYVEK

### Teszt 1: Conversational query (no RAG)
```
User: "szia"
Expected: 
  - needs_rag: false
  - final_answer: "Szia! Miben seg√≠thetek?"
  - sources: []
```

### Teszt 2: Document query (RAG path)
```
User: "mi van a dokumentumban?"
Expected:
  - needs_rag: true
  - Qdrant search aktiv√°l√≥dik
  - sources: [1, 2, 3] (ha van tal√°lat)
```

### Teszt 3: Ambiguous query
```
User: "√©rdekel"
Expected:
  - LLM decision alapj√°n routing
  - Ha conversational: no RAG
  - Ha document-related: RAG
```

### Teszt 4: Error handling
```
Scenario: LLM API timeout
Expected:
  - Default: needs_rag = true
  - Folytat√≥dik RAG path-on (konzervat√≠v fallback)
```

---

## üìö KAPCSOL√ìD√ì F√ÅJLOK

### Backend
- `backend/services/rag_workflow.py` (m√≥dos√≠tva)
  - `class RAGState` (√∫j field: needs_rag)
  - `_build_graph()` (√∫j edges)
  - `_decide_rag_needed_node()` (√∫j)
  - `_route_based_on_rag_decision()` (√∫j)
  - `_generate_direct_answer_node()` (√∫j)

### Frontend
- `frontend/src/api.ts` (v√°ltozatlan, POST /api/chat/rag)
- `frontend/src/types.ts` (v√°ltozatlan, sources?: number[])
- `frontend/src/components/MessageBubble.tsx` (v√°ltozatlan, source badges)

### Docs
- `docs/INIT_PROMPT_2.md` (P0.7 eredeti spec)
- `docs/P0.8_INTELLIGENT_RAG_ROUTING.md` (ez a dokumentum)

---

## üîÆ J√ñV≈êBELI FEJLESZT√âSEK (nem Phase 2.0)

### P0.9: Chat history context (optional)
- El≈ëz≈ë besz√©lget√©s figyelembe v√©tele
- "√©s mi a helyzet most?" ‚Üí context-aware routing

### P1.3: Multi-source routing (P1.3 szerint)
- Document vs Product collection routing
- Keyword-based (NEM LLM-based, spec szerint)
- Parallel retrieval

### P3: Fine-tuned intent classifier
- Custom model conversational vs RAG d√∂nt√©shez
- Gyorsabb mint GPT-3.5 call
- Olcs√≥bb m≈±k√∂d√©s

---

## üìä √ñSSZEFOGLAL√ÅS

| Aspektus | P0.7 (eredeti) | P0.8 (enhanced) |
|----------|----------------|-----------------|
| **Minden query RAG?** | ‚úÖ Igen | ‚ùå Nem, intelligens d√∂nt√©s |
| **Conversational support** | ‚ùå Nincs | ‚úÖ Van (generate_direct_answer) |
| **LLM decision** | ‚ùå Nincs | ‚úÖ decide_if_rag_needed node |
| **Gyorsabb v√°lasz** | ‚ùå Mindig teljes RAG | ‚úÖ Conversational gyors |
| **UX** | ‚ö†Ô∏è "Nincs dokumentum" k√∂sz√∂n√©sre | ‚úÖ Term√©szetes besz√©lget√©s |

**P0.8 = P0.7 + Intelligent Routing**

Ez a tov√°bbfejleszt√©s **nem √ºtk√∂zik** a specifik√°ci√≥val, hanem **kieg√©sz√≠ti** a RAG workflow-t egy sz√ºks√©ges UX feature-rel, amely lehet≈ëv√© teszi a rendszer sz√°m√°ra, hogy **norm√°l chat**-k√©nt is m≈±k√∂dj√∂n, nem csak dokumentum-keres≈ëk√©nt.
