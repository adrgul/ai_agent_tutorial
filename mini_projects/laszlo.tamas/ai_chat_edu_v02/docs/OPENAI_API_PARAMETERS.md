# OpenAI API Param√©terek - GPT-4 Viselked√©s M√≥dos√≠t√°sa

**Utols√≥ friss√≠t√©s**: 2026. janu√°r 2.

## üìã √Åttekint√©s

Ez a dokumentum √∂sszefoglalja az OpenAI GPT-4 (GPT-4 Turbo/GPT-4o) API h√≠v√°skor el√©rhet≈ë param√©tereket, amelyekkel m√≥dos√≠that√≥ a modell viselked√©se.

---

## üé® Kreativit√°s √©s V√©letlenszer≈±s√©g

### `temperature` (0.0 - 2.0)
- **Alap√©rtelmezett**: 1.0 (OpenAI), 0.7 (projekt√ºnk)
- **Le√≠r√°s**: Kimenet v√©letlenszer≈±s√©g√©nek m√©rt√©ke
- **√ârt√©kek**:
  - `0.0` = Teljesen determinisztikus, mindig ugyanaz a v√°lasz
  - `0.7` = Kiegyens√∫lyozott (aj√°nlott chat alkalmaz√°sokhoz)
  - `1.0` = K√∂zepes kreativit√°s
  - `2.0` = Nagyon kreat√≠v, v√°ratlan v√°laszok

**Haszn√°lat**:
```python
response = openai.chat.completions.create(
    model="gpt-4",
    messages=[...],
    temperature=0.7
)
```

### `top_p` (0.0 - 1.0)
- **Alap√©rtelmezett**: 1.0
- **Le√≠r√°s**: Nucleus sampling - alternat√≠va a temperature-h√∂z
- **M≈±k√∂d√©s**: Csak a legval√≥sz√≠n≈±bb tokenek haszn√°lata (kumulat√≠v val√≥sz√≠n≈±s√©g alapj√°n)
- **Aj√°nl√°s**: Ne haszn√°ld egyszerre a temperature-rel, v√°lassz egyet!

**P√©lda**:
- `top_p=0.9`: A legval√≥sz√≠n≈±bb 90%-nyi tokenek k√∂z√ºl v√°laszt

---

## üîÑ Ism√©tl√©s Kontroll

### `frequency_penalty` (-2.0 - 2.0)
- **Alap√©rtelmezett**: 0.0
- **Le√≠r√°s**: Cs√∂kkenti a m√°r haszn√°lt szavak ism√©tl≈ëd√©s√©t
- **M≈±k√∂d√©s**: A token eddigi el≈ëfordul√°si sz√°ma alapj√°n b√ºntet
- **Haszn√°lat**:
  - `0.5` - `1.0`: Kevesebb ism√©tl√©s
  - `1.0` - `2.0`: Er≈ësen ker√ºli az ism√©tl√©seket

### `presence_penalty` (-2.0 - 2.0)
- **Alap√©rtelmezett**: 0.0
- **Le√≠r√°s**: √ñszt√∂nzi √∫j t√©m√°k bevezet√©s√©t
- **M≈±k√∂d√©s**: Ha egy token m√°r el≈ëfordult (b√°rmilyen gyakran), b√ºntet√©s
- **K√ºl√∂nbs√©g**: Frequency figyelembe veszi a **mennyis√©get**, presence csak a **t√©nyt**

**P√©lda kombin√°lt haszn√°lat**:
```python
response = openai.chat.completions.create(
    model="gpt-4",
    messages=[...],
    frequency_penalty=0.5,  # Cs√∂kkenti az ism√©tl√©seket
    presence_penalty=0.6    # √öj t√©m√°k bevezet√©se
)
```

---

## üìè Kimenet Korl√°tok

### `max_tokens`
- **Alap√©rtelmezett**: Nincs (model maximum)
- **Le√≠r√°s**: Maxim√°lis gener√°lt tokenek sz√°ma
- **Figyelem**: Input + output √∂sszesen ne l√©pje t√∫l a modell context windowj√°t
  - GPT-4: 8192 token
  - GPT-4 Turbo: 128000 token
  - GPT-4o: 128000 token

### `stop`
- **T√≠pus**: String vagy string lista
- **Le√≠r√°s**: Stop szekvenci√°k, ahol a gener√°l√°s meg√°ll
- **P√©lda**:
```python
stop=["\n", "User:", "###"]  # Meg√°ll ezek valamelyik√©n√©l
```

---

## üîß Struktur√°lt Kimenet

### `response_format`
- **Lehet≈ës√©gek**: `{"type": "text"}` (alap√©rtelmezett) vagy `{"type": "json_object"}`
- **JSON m√≥d k√∂vetelm√©nye**: System prompt tartalmazza a "JSON" sz√≥t
- **Haszn√°lat**:
```python
response = openai.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "Return response as JSON"},
        {"role": "user", "content": "Extract name and age"}
    ],
    response_format={"type": "json_object"}
)
```

### `seed` (k√≠s√©rleti)
- **T√≠pus**: Integer
- **Le√≠r√°s**: Determinisztikus output biztos√≠t√°sa
- **Figyelem**: Nem 100% garant√°lt, de k√∂zel azonos v√°laszokat ad

---

## üõ†Ô∏è Function Calling / Tools

### `tools`
- **T√≠pus**: Lista objektumokb√≥l
- **Le√≠r√°s**: El√©rhet≈ë f√ºggv√©nyek/toolok defini√°l√°sa
- **Form√°tum**: JSON Schema alap√∫ le√≠r√°s

**P√©lda**:
```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                },
                "required": ["location"]
            }
        }
    }
]
```

### `tool_choice`
- **Lehet≈ës√©gek**: 
  - `"auto"` (alap√©rtelmezett): Model d√∂nt
  - `"none"`: Nem h√≠vhat f√ºggv√©nyt
  - `{"type": "function", "function": {"name": "konkr√©t_f√ºggv√©ny"}}`: K√©nyszer√≠t egy konkr√©t tool-t

---

## üìä Egy√©b Param√©terek

### `n`
- **Alap√©rtelmezett**: 1
- **Le√≠r√°s**: H√°ny alternat√≠v v√°laszt gener√°ljon p√°rhuzamosan
- **Figyelem**: K√∂lts√©ghat√©konys√°g! `n=3` = 3x token haszn√°lat

### `stream`
- **T√≠pus**: Boolean
- **Le√≠r√°s**: Streaming kimenet (chunk-onk√©nt √©rkeznek a tokenek)
- **Haszn√°lat**: Real-time UX √©lm√©ny (pl. ChatGPT-szer≈± g√©pel√©s effekt)

```python
stream = openai.chat.completions.create(
    model="gpt-4",
    messages=[...],
    stream=True
)
for chunk in stream:
    print(chunk.choices[0].delta.content, end="")
```

### `logprobs`
- **T√≠pus**: Boolean vagy integer
- **Le√≠r√°s**: Token-level val√≥sz√≠n≈±s√©gek visszaad√°sa
- **Haszn√°lat**: Debug, anal√≠zis, modell bizonytalans√°g√°nak m√©r√©se

---

## üéØ Projekt-Specifikus Konfigur√°ci√≥

Az `ai_chat_edu_v02` projektben:

**F√°jl**: `backend/config/system.ini`
```ini
CHAT_TEMPERATURE=0.7
CHAT_MODEL=gpt-3.5-turbo  # vagy gpt-4o-mini
```

**Haszn√°lat**: `services/unified_chat_workflow.py`
```python
llm = ChatOpenAI(
    model=self.model_name,
    temperature=self.temperature
)
```

---

## ÔøΩ Modell-Specifikus K√ºl√∂nbs√©gek

### ‚úÖ Minden modellben el√©rhet≈ë param√©terek
Az alapvet≈ë param√©terek **azonosak** minden GPT modellben:
- `temperature`, `top_p`, `max_tokens`, `stop`
- `frequency_penalty`, `presence_penalty`
- `n`, `stream`

### üìä Modell-f√ºgg≈ë funkci√≥k

#### GPT-3.5-turbo
- ‚ùå **Nincs vision** (k√©pfeldolgoz√°s)
- ‚úÖ **Function calling**: csak `gpt-3.5-turbo-1106+` verzi√≥kt√≥l
- ‚úÖ **JSON mode**: csak `gpt-3.5-turbo-1106+` verzi√≥kt√≥l
- ‚úÖ `seed`: √∫jabb verzi√≥kban el√©rhet≈ë
- ‚ö†Ô∏è `logprobs`: korl√°tozott (max 5 alternat√≠v token)
- **Context window**: 16K token (√∫jabb verzi√≥k)
- **K√∂lts√©g**: Legolcs√≥bb, gyors

#### GPT-4o-mini
- ‚úÖ **Vision t√°mogat√°s** (k√©pek √©rtelmez√©se)
- ‚úÖ **Function calling**: teljes t√°mogat√°s
- ‚úÖ **JSON mode**: teljes t√°mogat√°s
- ‚úÖ `seed`: el√©rhet≈ë
- ‚úÖ `logprobs`: b≈ëv√≠tett verzi√≥ (max 20 alternat√≠v token)
- **Context window**: 128K token
- **K√∂lts√©g**: K√∂z√©pkateg√≥ria, kiegyens√∫lyozott √°r/teljes√≠tm√©ny

#### GPT-4 / GPT-4 Turbo / GPT-4o
- ‚úÖ Minden funkci√≥ teljes t√°mogat√°ssal
- **Context window**: 128K token (Turbo/4o), 8K token (GPT-4)
- **K√∂lts√©g**: Legdr√°g√°bb, legjobb min≈ës√©g

### üéØ Gyakorlati k√ºl√∂nbs√©gek t√°bl√°zat

| Funkci√≥ | GPT-3.5-turbo | GPT-4o-mini | GPT-4 Turbo/4o |
|---------|---------------|-------------|----------------|
| **Alapparam√©terek** | ‚úÖ | ‚úÖ | ‚úÖ |
| **Function calling** | ‚úÖ (1106+) | ‚úÖ | ‚úÖ |
| **JSON mode** | ‚úÖ (1106+) | ‚úÖ | ‚úÖ |
| **Vision** | ‚ùå | ‚úÖ | ‚úÖ |
| **Max context** | 16K | 128K | 128K |
| **Logprobs (max)** | 5 | 20 | 20 |
| **Relat√≠v k√∂lts√©g** | 1x | 5x | 20x |

**Projekt konfigur√°ci√≥**: Az `ai_chat_edu_v02` projektben a `gpt-3.5-turbo` vagy `gpt-4o-mini` modellek mindkett≈ë t√°mogatj√°k az √∂sszes jelenleg haszn√°lt param√©tert.

---

## ÔøΩüí° Best Practices

1. **Temperature vs. Top_p**: V√°lassz egyet! Ne kombin√°ldd ≈ëket.
2. **Determinisztikus output**: `temperature=0` + `seed` param√©ter
3. **Kreat√≠v √≠r√°s**: `temperature=1.0-1.5` + `presence_penalty=0.6`
4. **K√≥d gener√°l√°s**: `temperature=0.2` + `max_tokens` limitelve
5. **JSON output**: Mindig adj p√©ld√°t a system promptban + `response_format`
6. **Token k√∂lts√©gek**: Figyelj a `max_tokens` √©s `n` param√©terre!

---

## üîó Kapcsol√≥d√≥ Dokumentumok

- [HIERARCHICAL_PROMPTS.md](HIERARCHICAL_PROMPTS.md) - Prompt hierarchia (Application ‚Üí Tenant ‚Üí User)
- [LANGGRAPH_WORKFLOWS.md](LANGGRAPH_WORKFLOWS.md) - LangGraph workflow architekt√∫ra
- `backend/config/system.ini` - Aktu√°lis konfigur√°ci√≥

---

## üìö Forr√°sok

- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/create)
- [OpenAI Best Practices](https://platform.openai.com/docs/guides/gpt-best-practices)
