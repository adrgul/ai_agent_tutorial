# System Configuration
# This file contains application-level settings that rarely change
# For environment-specific settings (credentials, hosts), use .env
#
# SEPARATION OF CONCERNS:
# - system.ini: Business logic, AI parameters, feature flags (version controlled)
# - .env: Secrets, infrastructure, ports (NOT in git, deployment-specific)
#
# Model Names (OPENAI_MODEL_CHAT, OPENAI_MODEL_EMBEDDING) are in .env
# because they may differ between environments (dev/staging/prod)

[application]
# Application version
APP_VERSION=0.2.0

# Language settings
DEFAULT_LANGUAGE=en

[development]
# Development mode - disables all caching layers for debugging
# WARNING: Set to false in production for optimal performance
DEV_MODE=false

# Context window management
MAX_CONTEXT_TOKENS=8000

# Application-level system prompt (applies to ALL tenants and users)
# This is the base instruction set for the AI assistant
# Hierarchy: system.ini > tenant.system_prompt > user.system_prompt
SYSTEM_PROMPT = Segítőkész személyes asszisztens vagy. Törekedj a rövid, határozott válaszokra. Csak a felhasználó külön kérésére térj el ettől. Ha valamiről nincs információd jelezd egyértelműen. Ne találj ki válaszokat. Csak az aktuális felhasználóról adhatsz ki személyes adatokat, semmilyenmás felhasználóról nem.

# Instructions for handling chat history (conversation context)
CHAT_HISTORY_INSTRUCTIONS = KRITIKUS SZABÁLY - FIGYELMESEN OLVASD EL: A "Chat History (Context)" alatt látható MINDEN üzenet (akár 30 darab) része a beszélgetésnek, függetlenül attól hogy különböző időpontokban vagy különböző session-ökben történtek. KÖTELEZŐ az ÖSSZES üzenetet elolvasni és megjegyezni! Példa: Ha a user azt kérdezi "mit tudsz a kutyámról?" és a history 15. üzeneténél azt írtam hogy "Teddy a neve, uszkár, 3 hónapos", akkor a válaszod: "Teddy a neved kutyád, uszkár fajtájú, 3 hónapos." - NEM MONDHATOD hogy "nem tudom" vagy "nem említetted"! Ha összefoglalót kér, MINDEN témát felsorolsz amit bármikor említettünk (kutya, autó, recept, bármi ami a 30 üzenetben szerepel).

[llm]
# Chat completion settings
CHAT_MAX_TOKENS=2000
CHAT_TEMPERATURE=0.7

[rag]
# RAG (Retrieval-Augmented Generation) pipeline settings
# FIGYELEM: Ezek a paraméterek NEM lehetnek hardcoded a kódban!

# Chunking
CHUNKING_STRATEGY=recursive
CHUNK_SIZE_TOKENS=500
CHUNK_OVERLAP_TOKENS=50

# Embedding
EMBEDDING_DIMENSIONS=3072
EMBEDDING_BATCH_SIZE=100

# Retrieval
TOP_K_DOCUMENTS=5
TOP_K_PRODUCTS=10
SIMILARITY_METRIC=cosine
MIN_SCORE_THRESHOLD=0.1

# Qdrant search
QDRANT_SEARCH_LIMIT=10
QDRANT_SEARCH_OFFSET=0

# Qdrant upload batching (to avoid 32 MB payload limit)
# Lower this value if you get "Payload error: JSON payload is larger than allowed" errors
QDRANT_UPLOAD_BATCH_SIZE=50

[memory]
# Long-term memory settings
ENABLE_LONGTERM_CHAT_STORAGE=true
ENABLE_LONGTERM_CHAT_RETRIEVAL=false
CHAT_SUMMARY_MAX_TOKENS=200
CONSOLIDATE_AFTER_MESSAGES=50
MIN_MESSAGES_FOR_CONSOLIDATION=5

# Retrieval settings (for future implementation)
TOP_K_LONG_TERM_MEMORIES=3
MEMORY_SCORE_THRESHOLD=0.5

# Short-term memory (recent messages sent to LLM for context)
SHORT_TERM_MEMORY_MESSAGES=30
# Scope: 'session' = only current session messages | 'user' = last N messages across all user sessions
SHORT_TERM_MEMORY_SCOPE=user

[limits]
# Limits
MAX_FILE_SIZE_MB=10
MAX_CHUNKS_PER_DOCUMENT=1000
MAX_DOCUMENTS_PER_USER=100

[rate_limiting]
# API rate limits
REQUESTS_PER_MINUTE=60
MAX_CONCURRENT_REQUESTS=10

[cache]
# ============================================================================
# CACHE ARCHITECTURE - 4 LAYERS (see docs/03_implementations/CACHE_ARCHITECTURE.md)
# ============================================================================

# --- TIER 1: In-Memory Cache (SimpleCache) ---
# Python dict-based cache for ultra-fast lookups (<1ms)
# Location: backend/services/cache_service.py
# Cached data: system prompts, tenant/user metadata
ENABLE_MEMORY_CACHE=true
MEMORY_CACHE_TTL_SECONDS=3600
MEMORY_CACHE_DEBUG=false

# --- TIER 2: PostgreSQL Database Cache (user_prompt_cache table) ---
# Persistent cache that survives container restarts (~10ms latency)
# Location: backend/database/pg_init.py (user_prompt_cache table)
# Cached data: Built system prompts per user
ENABLE_DB_CACHE=true
DB_CACHE_AUTO_CLEANUP=false

# --- TIER 3: Browser Cache (Frontend HTTP cache) ---
# Controls Cache-Control headers sent to browser
# Location: frontend API calls + backend response headers
# Applies to: Static assets, API responses
ENABLE_BROWSER_CACHE=true
BROWSER_CACHE_MAX_AGE_SECONDS=300

# --- TIER 4: LLM Prompt Caching (OpenAI API - TEMP.4) ---
# Server-side caching at OpenAI (5-10 min ephemeral cache)
# Status: ⏳ NOT YET IMPLEMENTED (see TODO_v02.md TEMP.4)
# Requires: GPT-4o model + cache_control parameter
ENABLE_LLM_CACHE=false
LLM_CACHE_PREFIX_TOKENS=250

[logging]
# Detailed logging settings
LOG_LLM_REQUESTS=true
LOG_VECTOR_SEARCHES=true
LOG_EMBEDDING_OPERATIONS=true
