# Prometheus & Grafana Monitoring √ötmutat√≥

**Teljes Programoz√°si √ötmutat√≥ AI Agent Megfigyelhet≈ës√©ghez**

Utols√≥ friss√≠t√©s: 2026. janu√°r 15.

---

## üìã Tartalomjegyz√©k

1. [√Åttekint√©s](#√°ttekint√©s)
2. [Architekt√∫ra √©s Adatfolyam](#architekt√∫ra-√©s-adatfolyam)
3. [Metrika T√≠pusok R√©szletesen](#metrika-t√≠pusok-r√©szletesen)
4. [Python Implement√°ci√≥](#python-implement√°ci√≥)
5. [LangGraph Integr√°ci√≥](#langgraph-integr√°ci√≥)
6. [Grafana Dashboard Konfigur√°ci√≥](#grafana-dashboard-konfigur√°ci√≥)
7. [Teljes P√©ld√°k](#teljes-p√©ld√°k)
8. [Legjobb Gyakorlatok](#legjobb-gyakorlatok)

---

## üìä √Åttekint√©s

### Mi ez a monitoring stack?

Az AI Agent projekt egy **h√°romszint≈± megfigyelhet≈ës√©gi stacket** haszn√°l:

```
Python Alkalmaz√°s (LangGraph Agent)
         ‚Üì (metrik√°kat r√∂gz√≠t)
    Prometheus Client Library
         ‚Üì (/metrics v√©gpontot szolg√°ltat)
    Prometheus Szerver (gy≈±jt √©s t√°rol)
         ‚Üì (lek√©rdez√©s PromQL-lel)
    Grafana Dashboardok (vizualiz√°l)
```

### F≈ëbb Komponensek

| Komponens | Szerep | Port | Technol√≥gia |
|-----------|--------|------|-------------|
| **Backend** | Metrik√°kat gener√°l az agent v√©grehajt√°s sor√°n | 8001 | FastAPI + LangGraph + prometheus_client |
| **Prometheus** | Gy≈±jt, id≈ësorozat adatokat t√°rol | 9090 | Prometheus TSDB |
| **Grafana** | Metrik√°kat vizualiz√°l dashboardokon | 3001 | Grafana |

---

## üèóÔ∏è Architekt√∫ra √©s Adatfolyam

### Teljes Monitoring Folyamat

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 1. FELHASZN√ÅL√ì √úZENETET K√úLD                    ‚îÇ
‚îÇ               "Milyen id≈ë van Budapesten?"                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              2. LANGGRAPH AGENT FELDOLGOZZA A K√âR√âST            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Node: agent_decide                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üí record_node_duration("agent_decide") id≈ëz√≠t≈ë indul    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üí instrumented_llm_call() LLM metrik√°kat r√∂gz√≠t:        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ llm_inference_count +1                             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ llm_inference_token_input_total +450               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ llm_inference_token_output_total +85               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ llm_inference_latency_seconds 1.2s                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ llm_cost_total_usd +$0.015                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üí record_node_duration() befejez≈ëdik:                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ node_execution_latency_seconds{node="agent_decide"}‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚Üì                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Node: tool_execution                                    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üí record_tool_call("weather") id≈ëz√≠t≈ë indul             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üí Weather API h√≠v√°s                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üí record_tool_call() befejez≈ëdik:                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ tool_invocation_count{tool="weather"} +1           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ     ‚Ä¢ agent_tool_duration_seconds{tool="weather"} 0.8s   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                         ‚Üì                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Agent befejezi a v√©grehajt√°st                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üí agent_execution_count +1                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üí agent_execution_latency_seconds 2.5s                  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       3. BACKEND KISZOLG√ÅLJA A METRIK√ÅKAT /metrics V√âGPONTON    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  GET http://localhost:8001/metrics visszaad:                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  # HELP llm_inference_count √ñsszes LLM inferencia h√≠v√°s        ‚îÇ
‚îÇ  # TYPE llm_inference_count counter                            ‚îÇ
‚îÇ  llm_inference_count{model="gpt-4o-mini"} 1.0                  ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  # HELP llm_inference_token_input_total Bemeneti tokenek       ‚îÇ
‚îÇ  # TYPE llm_inference_token_input_total counter                ‚îÇ
‚îÇ  llm_inference_token_input_total{model="gpt-4o-mini"} 450.0    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  # HELP tool_invocation_count Eszk√∂z h√≠v√°sok                   ‚îÇ
‚îÇ  # TYPE tool_invocation_count counter                          ‚îÇ
‚îÇ  tool_invocation_count{tool="weather"} 1.0                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ... (√©s 20+ tov√°bbi metrika)                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      4. PROMETHEUS LEGY≈∞JTI A /metrics-t 15 M√ÅSODPERCENK√âNT     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Prometheus konfigur√°ci√≥ (prometheus.yml):                     ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  scrape_configs:                                               ‚îÇ
‚îÇ    - job_name: 'ai-agent'                                      ‚îÇ
‚îÇ      static_configs:                                           ‚îÇ
‚îÇ        - targets: ['ai-agent-backend:8000']                    ‚îÇ
‚îÇ      scrape_interval: 15s                                      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Üí Prometheus t√°rolja a metrik√°kat id≈ësorozat adatb√°zisban     ‚îÇ
‚îÇ  ‚Üí Meg≈ërz√©s: 15 nap vagy 10GB                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          5. GRAFANA LEK√âRDEZI A PROMETHEUS-t PromQL-lel         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Dashboard Panel Konfigur√°ci√≥:                                 ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Lek√©rdez√©s: rate(llm_inference_count[5m])                     ‚îÇ
‚îÇ  Jelmagyar√°zat: {{model}}                                      ‚îÇ
‚îÇ  Vizualiz√°ci√≥: Id≈ësorozat Grafikon                             ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Üí Grafana lek√©ri az adatokat Prometheus-b√≥l 5-30s-enk√©nt      ‚îÇ
‚îÇ  ‚Üí Interakt√≠v grafikonokat jelen√≠t meg zoom, pan, id≈ëtartom√°ny ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìè Metrika T√≠pusok R√©szletesen

A Prometheus **4 f≈ë metrika t√≠pust** t√°mogat. Meg√©rt√©s√ºk kritikus a helyes instrument√°ci√≥hoz.

### 1. Counter (Sz√°ml√°l√≥)

**Defin√≠ci√≥**: Monoton n√∂vekv≈ë √©rt√©k, ami csak n≈ë (vagy √∫jraindul null√°ra restart eset√©n).

**Mikor haszn√°ljuk**: Esem√©nyek sz√°mol√°sa, amik id≈ëvel felhalmoz√≥dnak.

**Python P√©lda**:
```python
from prometheus_client import Counter

# Sz√°ml√°l√≥ defini√°l√°sa
llm_inference_count = Counter(
    name='llm_inference_count',
    documentation='√ñsszes LLM inferencia h√≠v√°s',
    labelnames=['model']  # C√≠mk√©k csoportos√≠t√°shoz
)

# Sz√°ml√°l√≥ haszn√°lata
llm_inference_count.labels(model="gpt-4o-mini").inc()  # N√∂vel√©s 1-gyel
llm_inference_count.labels(model="gpt-4o-mini").inc(5)  # N√∂vel√©s 5-tel
```

**Val√≥s p√©lda a projektb≈ël**:
```python
# F√°jl: backend/observability/metrics.py

tool_invocation_count = Counter(
    name='tool_invocation_count',
    documentation='√ñsszes eszk√∂z h√≠v√°s',
    labelnames=['tool']
)

# Haszn√°lat a backend/services/tools.py-ban:
with record_tool_call("weather"):
    result = await self.client.get_forecast(...)
    # Bel√ºl n√∂veli: tool_invocation_count{tool="weather"}
```

**PromQL lek√©rdez√©sek sz√°ml√°l√≥khoz**:
```promql
# Teljes sz√°m
llm_inference_count{model="gpt-4o-mini"}

# M√°sodpercenk√©nti ar√°ny 5 perc alatt
rate(llm_inference_count[5m])

# Teljes n√∂veked√©s 1 √≥ra alatt
increase(llm_inference_count[1h])
```

---

### 2. Gauge (M√©r≈ë)

**Defin√≠ci√≥**: √ârt√©k, ami n≈ëhet vagy cs√∂kkenhet (aktu√°lis √°llapot/szint).

**Mikor haszn√°ljuk**: Aktu√°lis √©rt√©kek m√©r√©se, mint h≈ëm√©rs√©klet, mem√≥riahaszn√°lat, sorhossz.

**Python P√©lda**:
```python
from prometheus_client import Gauge

# M√©r≈ë defini√°l√°sa
active_connections = Gauge(
    name='active_connections',
    documentation='Akt√≠v kapcsolatok jelenlegi sz√°ma'
)

# M√©r≈ë haszn√°lata
active_connections.set(42)  # Be√°ll√≠t√°s konkr√©t √©rt√©kre
active_connections.inc()    # N√∂vel√©s 1-gyel
active_connections.dec(5)   # Cs√∂kkent√©s 5-tel
```

**Val√≥s p√©lda a projektb≈ël**:
```python
# F√°jl: backend/observability/metrics.py

rag_recall_rate = Gauge(
    name='rag_recall_rate',
    documentation='RAG visszah√≠v√°si ar√°ny (sz√°rmaztatott relevancia metrika)'
)

# Haszn√°lat:
rag_recall_rate.set(0.87)  # 87% visszah√≠v√°si ar√°ny
```

**PromQL lek√©rdez√©sek m√©r≈ëkh√∂z**:
```promql
# Jelenlegi √©rt√©k
rag_recall_rate

# √Åtlag id≈ëben
avg_over_time(rag_recall_rate[5m])

# Maximum √©rt√©k az elm√∫lt √≥r√°ban
max_over_time(rag_recall_rate[1h])
```

---

### 3. Histogram (Hisztogram)

**Defin√≠ci√≥**: Megfigyel√©seket mint√°z √©s konfigur√°lhat√≥ kos√°rba sz√°molja ≈ëket. Automatikusan szolg√°ltat count, sum √©s kvantilis sz√°m√≠t√°sokat.

**Mikor haszn√°ljuk**: Eloszl√°sok m√©r√©se (k√©sleltet√©s, k√©r√©s m√©retek, id≈ëtartamok).

**Python P√©lda**:
```python
from prometheus_client import Histogram

# Hisztogram defini√°l√°sa
request_duration_seconds = Histogram(
    name='request_duration_seconds',
    documentation='K√©r√©s id≈ëtartama m√°sodpercben',
    labelnames=['method'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]  # Egy√©ni kosarak
)

# Hisztogram haszn√°lata
request_duration_seconds.labels(method="GET").observe(1.2)  # 1.2 mp r√∂gz√≠t√©se
```

**Val√≥s p√©lda a projektb≈ël**:
```python
# F√°jl: backend/observability/metrics.py

llm_inference_latency_seconds = Histogram(
    name='llm_inference_latency_seconds',
    documentation='LLM inferencia h√≠v√°sok k√©sleltet√©se m√°sodpercben',
    labelnames=['model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

# Haszn√°lat a backend/observability/llm_instrumentation.py-ban:
start_time = time.time()
response = await llm.ainvoke(messages)
duration = time.time() - start_time
llm_inference_latency_seconds.labels(model="gpt-4o-mini").observe(duration)
```

**Mit hoz l√©tre a hisztogram**:
```
# H√°rom metrika automatikusan gener√°l√≥dik:
llm_inference_latency_seconds_bucket{model="gpt-4o-mini",le="0.5"} 10
llm_inference_latency_seconds_bucket{model="gpt-4o-mini",le="1.0"} 45
llm_inference_latency_seconds_bucket{model="gpt-4o-mini",le="5.0"} 98
llm_inference_latency_seconds_sum{model="gpt-4o-mini"} 123.4
llm_inference_latency_seconds_count{model="gpt-4o-mini"} 100
```

**PromQL lek√©rdez√©sek hisztogramokhoz**:
```promql
# 95. percentilis k√©sleltet√©s sz√°m√≠t√°sa
histogram_quantile(0.95, rate(llm_inference_latency_seconds_bucket[5m]))

# 50. percentilis (medi√°n) sz√°m√≠t√°sa
histogram_quantile(0.50, rate(llm_inference_latency_seconds_bucket[5m]))

# √Åtlag k√©sleltet√©s
rate(llm_inference_latency_seconds_sum[5m]) / rate(llm_inference_latency_seconds_count[5m])
```

---

### 4. Summary (√ñsszegz√©s)

**Defin√≠ci√≥**: Hasonl√≥ a hisztogramhoz, de kliensoldali kvantilisokat sz√°mol.

**Mikor haszn√°ljuk**: Ha pontos kvantilisokra van sz√ºks√©g, de nincs sz√ºks√©g p√©ld√°nyok k√∂z√∂tti aggreg√°ci√≥ra.

**Megjegyz√©s**: Ez a projekt **Hisztogramokat** haszn√°l √ñsszegz√©sek helyett, mert a hisztogramok rugalmasabbak elosztott rendszerekben t√∂rt√©n≈ë aggreg√°ci√≥hoz.

---

## üêç Python Implement√°ci√≥

### 1. L√©p√©s: F√ºgg≈ës√©gek Telep√≠t√©se

```bash
# A backend/requirements.txt-ben
prometheus-client==0.19.0
```

### 2. L√©p√©s: Metrik√°k Defini√°l√°sa

**F√°jl: `backend/observability/metrics.py`**

```python
import os
from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry

# Registry l√©trehoz√°sa
registry = CollectorRegistry()

# Metrik√°k defini√°l√°sa
llm_inference_count = Counter(
    name='llm_inference_count',
    documentation='√ñsszes LLM inferencia h√≠v√°s',
    labelnames=['model'],
    registry=registry
)

llm_inference_latency_seconds = Histogram(
    name='llm_inference_latency_seconds',
    documentation='LLM inferencia h√≠v√°sok k√©sleltet√©se m√°sodpercben',
    labelnames=['model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0],
    registry=registry
)

tool_invocation_count = Counter(
    name='tool_invocation_count',
    documentation='√ñsszes eszk√∂z h√≠v√°s',
    labelnames=['tool'],
    registry=registry
)
```

### 3. L√©p√©s: Metrik√°k R√∂gz√≠t√©se a K√≥dban

#### 1. P√©lda: LLM H√≠v√°sok R√∂gz√≠t√©se

**F√°jl: `backend/observability/llm_instrumentation.py`**

```python
import time
from observability.metrics import (
    llm_inference_count,
    llm_inference_latency_seconds,
    llm_inference_token_input_total,
    llm_inference_token_output_total,
    llm_cost_total_usd
)

async def instrumented_llm_call(llm, messages, model: str):
    """Wrapper, ami automatikusan r√∂gz√≠ti az LLM metrik√°kat."""
    
    # Id≈ëz√≠t√©s ind√≠t√°sa
    start_time = time.time()
    
    try:
        # T√©nyleges LLM h√≠v√°s
        response = await llm.ainvoke(messages)
        
        # Id≈ëtartam sz√°m√≠t√°sa
        duration = time.time() - start_time
        
        # Token haszn√°lat kinyer√©se
        prompt_tokens = response.usage_metadata.get('input_tokens', 0)
        completion_tokens = response.usage_metadata.get('output_tokens', 0)
        
        # Metrik√°k r√∂gz√≠t√©se
        llm_inference_count.labels(model=model).inc()
        llm_inference_latency_seconds.labels(model=model).observe(duration)
        llm_inference_token_input_total.labels(model=model).inc(prompt_tokens)
        llm_inference_token_output_total.labels(model=model).inc(completion_tokens)
        
        # K√∂lts√©g sz√°m√≠t√°sa √©s r√∂gz√≠t√©se
        cost = calculate_cost(model, prompt_tokens, completion_tokens)
        llm_cost_total_usd.labels(model=model).inc(cost)
        
        return response
        
    except Exception as e:
        # Hiba metrik√°k r√∂gz√≠t√©se
        duration = time.time() - start_time
        llm_inference_latency_seconds.labels(model=model).observe(duration)
        raise
```

#### 2. P√©lda: Eszk√∂z H√≠v√°sok R√∂gz√≠t√©se Context Manager-rel

**F√°jl: `backend/observability/metrics.py`**

```python
import time
from contextlib import contextmanager
from observability.metrics import tool_invocation_count, agent_tool_duration_seconds

@contextmanager
def record_tool_call(tool_name: str):
    """
    Context manager eszk√∂z h√≠v√°s metrik√°k r√∂gz√≠t√©s√©hez.
    
    Haszn√°lat:
        with record_tool_call("weather"):
            result = await weather_api.get_forecast()
    """
    start_time = time.time()
    
    try:
        yield
        # Sikeres √∫tvonal
        tool_invocation_count.labels(tool=tool_name).inc()
    except Exception as e:
        # Hiba √∫tvonal - h√≠v√°st tov√°bbra is r√∂gz√≠tj√ºk
        tool_invocation_count.labels(tool=tool_name).inc()
        raise
    finally:
        # Mindig r√∂gz√≠tj√ºk az id≈ëtartamot
        duration = time.time() - start_time
        agent_tool_duration_seconds.labels(
            tool=tool_name,
            environment="dev"
        ).observe(duration)
```

**Haszn√°lat a `backend/services/tools.py`-ban**:

```python
from observability.metrics import record_tool_call

class WeatherTool:
    async def execute(self, city: str):
        with record_tool_call("weather"):
            logger.info(f"Weather API h√≠v√°s: {city}")
            result = await self.client.get_forecast(city=city)
            return result
```

#### 3. P√©lda: Node V√©grehajt√°si Id≈ëk R√∂gz√≠t√©se

**F√°jl: `backend/observability/metrics.py`**

```python
@contextmanager
def record_node_duration(node_name: str):
    """
    Context manager LangGraph node v√©grehajt√°si id≈ë r√∂gz√≠t√©s√©hez.
    
    Haszn√°lat:
        with record_node_duration("agent_decide"):
            state = await _agent_decide_node(state)
    """
    start_time = time.time()
    
    try:
        yield
    finally:
        duration = time.time() - start_time
        node_execution_latency_seconds.labels(node=node_name).observe(duration)
        agent_node_executions_total.labels(
            node=node_name,
            environment="dev"
        ).inc()
```

### 4. L√©p√©s: Metrika V√©gpont Kiszolg√°l√°sa

**F√°jl: `backend/main.py`**

```python
from fastapi import FastAPI
from prometheus_client import make_asgi_app, generate_latest, CONTENT_TYPE_LATEST
from observability.metrics import registry, init_metrics

app = FastAPI()

# Metrik√°k inicializ√°l√°sa metaadatokkal
init_metrics(environment="dev", version="1.0.0")

# Prometheus metrika v√©gpont csatol√°sa a /metrics c√≠men
metrics_app = make_asgi_app(registry=registry)
app.mount("/metrics", metrics_app)

# Vagy manu√°lis v√©gpont:
@app.get("/metrics")
async def metrics():
    from starlette.responses import Response
    return Response(
        content=generate_latest(registry),
        media_type=CONTENT_TYPE_LATEST
    )
```

### 5. L√©p√©s: Metrika V√©gpont Tesztel√©se

```bash
# Backend ind√≠t√°sa
docker-compose up -d backend

# Teszt k√©r√©s k√ºld√©se metrik√°k gener√°l√°s√°hoz
curl -X POST http://localhost:8001/v1/agent/chat \
  -H "Content-Type: application/json" \
  -d '{"user_id": "test", "message": "Milyen id≈ë van?"}'

# Metrika v√©gpont ellen≈ërz√©se
curl http://localhost:8001/metrics

# Kimenet:
# HELP llm_inference_count √ñsszes LLM inferencia h√≠v√°s
# TYPE llm_inference_count counter
llm_inference_count{model="gpt-4o-mini"} 1.0
# HELP tool_invocation_count √ñsszes eszk√∂z h√≠v√°s
# TYPE tool_invocation_count counter
tool_invocation_count{tool="weather"} 1.0
```

---

## üîó LangGraph Integr√°ci√≥

### Hogyan R√∂gz√≠tik a LangGraph Node-ok a Metrik√°kat

**F√°jl: `backend/services/agent.py`**

```python
from langgraph.graph import StateGraph, END
from observability.metrics import record_node_duration
from observability.llm_instrumentation import instrumented_llm_call

class AIAgent:
    def __init__(self, llm, tools):
        self.llm = llm
        self.tools = tools
        self.graph = self._build_graph()
    
    def _build_graph(self):
        workflow = StateGraph(AgentState)
        
        # Node-ok hozz√°ad√°sa metrika instrument√°ci√≥val
        workflow.add_node("agent_decide", self._agent_decide_node)
        workflow.add_node("tool_execution", self._tool_execution_node)
        workflow.add_node("finalize", self._finalize_node)
        
        # √âlek hozz√°ad√°sa...
        return workflow.compile()
    
    async def _agent_decide_node(self, state: AgentState) -> AgentState:
        """
        LangGraph node, ami LLM d√∂nt√©st hoz.
        R√∂gz√≠ti: node v√©grehajt√°si id≈ë, LLM metrik√°k
        """
        with record_node_duration("agent_decide"):
            # √úzenetek el≈ëk√©sz√≠t√©se
            messages = self._prepare_messages(state)
            
            # LLM h√≠v√°s instrument√°ci√≥val
            response = await instrumented_llm_call(
                llm=self.llm,
                messages=messages,
                model="gpt-4o-mini",
                agent_execution_id=state.get("agent_execution_id")
            )
            
            # D√∂nt√©s feldolgoz√°sa
            decision = self._parse_decision(response.content)
            
            return {
                **state,
                "last_decision": decision,
                "messages": state["messages"] + [response]
            }
    
    async def _tool_execution_node(self, state: AgentState) -> AgentState:
        """
        LangGraph node, ami eszk√∂z√∂ket hajt v√©gre.
        R√∂gz√≠ti: node v√©grehajt√°si id≈ë, eszk√∂z h√≠v√°s metrik√°k
        """
        with record_node_duration("tool_execution"):
            decision = state["last_decision"]
            tool_name = decision["tool_name"]
            arguments = decision["arguments"]
            
            # Eszk√∂z keres√©se √©s v√©grehajt√°sa (az eszk√∂z m√°r r√∂gz√≠ti saj√°t metrik√°it)
            tool = self.tools[tool_name]
            result = await tool.execute(**arguments)
            
            return {
                **state,
                "tool_results": state.get("tool_results", []) + [result]
            }
```

### Teljes Folyamat P√©lda

```python
# Felhaszn√°l√≥ k√ºldi: "Milyen id≈ë van Budapesten?"

# 1. Agent graph elind√≠tja a v√©grehajt√°st
#    ‚Üí agent_execution_count.inc()
#    ‚Üí Id≈ëz√≠t≈ë indul az agent_execution_latency_seconds-hez

# 2. Node: agent_decide
#    with record_node_duration("agent_decide"):  # ‚Üê Node id≈ëz√≠t≈ë indul
#        response = await instrumented_llm_call(...)  # ‚Üê LLM metrik√°k r√∂gz√≠tve
#        # Az instrumented_llm_call() belsej√©ben:
#        #   llm_inference_count.labels(model="gpt-4o-mini").inc()
#        #   llm_inference_token_input_total.labels(model="gpt-4o-mini").inc(450)
#        #   llm_inference_token_output_total.labels(model="gpt-4o-mini").inc(85)
#        #   llm_inference_latency_seconds.labels(model="gpt-4o-mini").observe(1.2)
#        #   llm_cost_total_usd.labels(model="gpt-4o-mini").inc(0.015)
#    # Node id≈ëz√≠t≈ë v√©ge:
#    #   node_execution_latency_seconds.labels(node="agent_decide").observe(1.3)

# 3. Node: tool_execution
#    with record_node_duration("tool_execution"):  # ‚Üê Node id≈ëz√≠t≈ë indul
#        with record_tool_call("weather"):  # ‚Üê Eszk√∂z id≈ëz√≠t≈ë indul
#            result = await weather_client.get_forecast(city="Budapest")
#        # Eszk√∂z id≈ëz√≠t≈ë v√©ge:
#        #   tool_invocation_count.labels(tool="weather").inc()
#        #   agent_tool_duration_seconds.labels(tool="weather").observe(0.8)
#    # Node id≈ëz√≠t≈ë v√©ge:
#    #   node_execution_latency_seconds.labels(node="tool_execution").observe(0.9)

# 4. Agent befejez≈ëdik
#    ‚Üí agent_execution_latency_seconds.observe(2.5)
```

---

## üìà Grafana Dashboard Konfigur√°ci√≥

### Hogyan Jelen√≠thetj√ºk Meg a Metrik√°kat a Grafan√°ban

#### 1. L√©p√©s: Adatforr√°s Konfigur√°l√°sa

**F√°jl: `observability/grafana/provisioning/datasources/prometheus.yml`**

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    jsonData:
      timeInterval: "15s"
```

#### 2. L√©p√©s: Dashboard Panel L√©trehoz√°sa

**P√©lda: LLM Inferencia Sz√°m Grafikon**

```json
{
  "title": "LLM Inferencia Sz√°m (ar√°ny)",
  "type": "timeseries",
  "datasource": {
    "type": "prometheus",
    "uid": "PBFA97CFB590B2093"
  },
  "targets": [
    {
      "expr": "rate(llm_inference_count[5m])",
      "legendFormat": "{{model}}",
      "refId": "A"
    }
  ],
  "fieldConfig": {
    "defaults": {
      "unit": "ops",
      "custom": {
        "lineWidth": 2,
        "fillOpacity": 10
      }
    }
  }
}
```

#### 3. L√©p√©s: Gyakori PromQL Lek√©rdez√©sek Dashboardokhoz

##### LLM Dashboard Panelek

**Panel 1: LLM Inferencia Ar√°ny**
```promql
# Lek√©rdez√©s:
rate(llm_inference_count[5m])

# Jelmagyar√°zat: {{model}}
# Vizualiz√°ci√≥: Id≈ësorozat
# Egys√©g: ops (m≈±veletek m√°sodpercenk√©nt)
```

**Panel 2: Token Haszn√°lat Modell Szerint**
```promql
# Bemeneti tokenek:
rate(llm_inference_token_input_total[5m])

# Kimeneti tokenek:
rate(llm_inference_token_output_total[5m])

# Jelmagyar√°zat: {{model}} - bemenet/kimenet
# Vizualiz√°ci√≥: Halmozott Ter√ºletdiagram
# Egys√©g: tokenek/mp
```

**Panel 3: LLM K√©sleltet√©s Percentilisek**
```promql
# p50 (medi√°n):
histogram_quantile(0.50, rate(llm_inference_latency_seconds_bucket[5m]))

# p95:
histogram_quantile(0.95, rate(llm_inference_latency_seconds_bucket[5m]))

# p99:
histogram_quantile(0.99, rate(llm_inference_latency_seconds_bucket[5m]))

# Jelmagyar√°zat: p50, p95, p99
# Vizualiz√°ci√≥: Id≈ësorozat t√∂bb lek√©rdez√©ssel
# Egys√©g: m√°sodpercek
```

**Panel 4: Teljes K√∂lts√©g**
```promql
# Lek√©rdez√©s:
sum(llm_cost_total_usd)

# Vizualiz√°ci√≥: Stat (egyetlen sz√°m)
# Egys√©g: currency USD ($)
```

##### Agent Munkafolyamat Dashboard Panelek

**Panel 5: Eszk√∂z H√≠v√°s R√©szletez√©s**
```promql
# Lek√©rdez√©s:
sum by (tool) (increase(tool_invocation_count[5m]))

# Jelmagyar√°zat: {{tool}}
# Vizualiz√°ci√≥: Oszlopdiagram vagy K√∂rdiagram
# Egys√©g: short (darabsz√°m)
```

**Panel 6: Node V√©grehajt√°si K√©sleltet√©s**
```promql
# √Åtlagos k√©sleltet√©s node-onk√©nt:
sum by (node) (rate(node_execution_latency_seconds_sum[5m])) 
  / 
sum by (node) (rate(node_execution_latency_seconds_count[5m]))

# Jelmagyar√°zat: {{node}}
# Vizualiz√°ci√≥: Id≈ësorozat
# Egys√©g: m√°sodpercek
```

**Panel 7: Agent V√©grehajt√°s Sz√°m**
```promql
# Lek√©rdez√©s:
increase(agent_execution_count[5m])

# Vizualiz√°ci√≥: Stat vagy Id≈ësorozat
# Egys√©g: short
```

##### K√∂lts√©g Dashboard Panelek

**Panel 8: K√∂lts√©g Modell Szerint (Utols√≥ √ìra)**
```promql
# Lek√©rdez√©s:
sum by (model) (increase(llm_cost_total_usd[1h]))

# Jelmagyar√°zat: {{model}}
# Vizualiz√°ci√≥: K√∂rdiagram
# Egys√©g: currency USD ($)
```

**Panel 9: K√∂lt√©si √útem (K√∂lts√©g Naponta)**
```promql
# Lek√©rdez√©s:
sum(increase(llm_cost_total_usd[24h]))

# Vizualiz√°ci√≥: Stat trenddel
# Egys√©g: currency USD ($)
```

**Panel 10: K√∂lts√©g Munkafolyamatonk√©nt**
```promql
# Lek√©rdez√©s:
sum(increase(llm_cost_total_usd[5m])) 
  / 
sum(increase(agent_execution_count[5m]))

# Vizualiz√°ci√≥: M√©r≈ë
# Egys√©g: currency USD ($)
```

---

## üî¨ Teljes P√©ld√°k

### 1. P√©lda: √öj Metrika Hozz√°ad√°sa

**Forgat√≥k√∂nyv**: K√∂vetj√ºk, h√°nyszor √°ll√≠tj√°k vissza a felhaszn√°l√≥k a besz√©lget√©si kontextust.

#### 1. L√©p√©s: Metrika Defini√°l√°sa

**F√°jl: `backend/observability/metrics.py`**

```python
context_reset_count = Counter(
    name='context_reset_count',
    documentation='Kontextus vissza√°ll√≠t√°sok teljes sz√°ma',
    labelnames=['user_id'],  # ‚ö†Ô∏è √ìvatosan haszn√°land√≥ - magas kardinalit√°st okozhat
    registry=registry
)
```

#### 2. L√©p√©s: Metrika R√∂gz√≠t√©se

**F√°jl: `backend/services/chat_service.py`**

```python
from observability.metrics import context_reset_count

class ChatService:
    async def process_message(self, user_id: str, message: str):
        # Reset parancs √©szlel√©se
        if message.lower().strip() == "reset context":
            # Session t√∂rl√©se
            await self.conversation_repo.clear_session(user_id)
            
            # Metrika r√∂gz√≠t√©se
            context_reset_count.labels(user_id=user_id).inc()
            
            return "Kontextus sikeresen vissza√°ll√≠tva"
```

#### 3. L√©p√©s: Grafana Panel L√©trehoz√°sa

```json
{
  "title": "Kontextus Vissza√°ll√≠t√°sok (Utols√≥ √ìra)",
  "type": "stat",
  "targets": [
    {
      "expr": "sum(increase(context_reset_count[1h]))",
      "refId": "A"
    }
  ]
}
```

---

### 2. P√©lda: T√∂bbdimenzi√≥s Metrik√°k

**Forgat√≥k√∂nyv**: Eszk√∂z sikeres vs. sikertelen ar√°nyok k√∂vet√©se eszk√∂z√∂nk√©nt.

#### 1. L√©p√©s: St√°tusz C√≠mke Hozz√°ad√°sa

**F√°jl: `backend/observability/metrics.py`**

```python
@contextmanager
def record_tool_call(tool_name: str):
    start_time = time.time()
    status = "success"  # Alap√©rtelmezett
    
    try:
        yield
    except Exception as e:
        status = "error"
        raise
    finally:
        duration = time.time() - start_time
        
        # R√∂gz√≠t√©s st√°tusz c√≠mk√©vel
        tool_invocation_count.labels(
            tool=tool_name,
            status=status  # ‚Üê St√°tusz dimenzi√≥ hozz√°adva
        ).inc()
        
        agent_tool_duration_seconds.labels(
            tool=tool_name
        ).observe(duration)
```

#### 2. L√©p√©s: Metrika Defin√≠ci√≥ Friss√≠t√©se

```python
tool_invocation_count = Counter(
    name='tool_invocation_count',
    documentation='√ñsszes eszk√∂z h√≠v√°s',
    labelnames=['tool', 'status'],  # ‚Üê St√°tusz c√≠mke hozz√°adva
    registry=registry
)
```

#### 3. L√©p√©s: Lek√©rdez√©s Grafan√°ban

```promql
# Sikeres ar√°ny:
sum by (tool) (rate(tool_invocation_count{status="success"}[5m]))
  / 
sum by (tool) (rate(tool_invocation_count[5m]))

# Hiba sz√°m:
sum by (tool) (increase(tool_invocation_count{status="error"}[5m]))
```

---

## üéØ Legjobb Gyakorlatok

### 1. C√≠mke Kardinalit√°s

**‚ùå ROSSZ** - Magas kardinalit√°s (c√≠mkekombin√°ci√≥k milli√≥i):
```python
request_count = Counter(
    'request_count',
    '√ñsszes k√©r√©s',
    labelnames=['user_id', 'session_id', 'request_id']  # ‚ùå T√∫l sok egyedi √©rt√©k!
)
```

**‚úÖ J√ì** - Alacsony kardinalit√°s (korl√°tozott egyedi √©rt√©kek):
```python
request_count = Counter(
    'request_count',
    '√ñsszes k√©r√©s',
    labelnames=['status', 'endpoint']  # ‚úÖ Kev√©s egyedi √©rt√©k
)
```

**Szab√°ly**: A c√≠mk√©knek c√≠mk√©nk√©nt **< 100 egyedi √©rt√©kkel** kell rendelkezni√ºk.

---

### 2. Hisztogram Haszn√°lata K√©sleltet√©shez

**‚ùå ROSSZ** - √Åtlag haszn√°lata sz√°ml√°l√≥b√≥l:
```python
total_duration = Counter('total_duration', 'Teljes id≈ëtartam')
request_count = Counter('request_count', 'K√©r√©sek')

# √Åtlag = total_duration / request_count  # ‚ùå Elvesz√≠ti az eloszl√°s inform√°ci√≥t
```

**‚úÖ J√ì** - Hisztogram haszn√°lata:
```python
request_duration = Histogram(
    'request_duration_seconds',
    'K√©r√©s id≈ëtartam',
    buckets=[0.1, 0.5, 1.0, 5.0, 10.0]  # ‚úÖ Percentilisek sz√°m√≠that√≥k
)
```

---

### 3. Elnevez√©si Konvenci√≥k

K√∂vess√ºk a Prometheus elnevez√©si konvenci√≥kat:

```python
# ‚úÖ J√ì nevek:
llm_inference_count            # Sz√°ml√°l√≥ - _total vagy _count v√©gz≈ëd√©ssel
llm_inference_latency_seconds  # Hisztogram - egys√©get tartalmaz (_seconds)
active_connections             # M√©r≈ë - jelenlegi √°llapotot √≠r le

# ‚ùå ROSSZ nevek:
LLMInferenceCount             # snake_case-t haszn√°lj, ne CamelCase-t
llm_latency_ms                # Alapegys√©geket haszn√°lj (m√°sodperc, nem ezredm√°sodperc)
tool_calls_total_count        # Redund√°ns (_total m√°r jelzi a count-ot)
```

---

### 4. Metrika Hat√°sk√∂r

**Glob√°lisan defini√°lj, lok√°lisan haszn√°lj**:

```python
# ‚úÖ J√ì - Egyszer defini√°lva a metrics.py-ban
# backend/observability/metrics.py
request_count = Counter('request_count', 'K√©r√©sek')

# Haszn√°ld mindenhol
# backend/services/agent.py
from observability.metrics import request_count
request_count.inc()

# ‚ùå ROSSZ - Ne defini√°ld √∫jra t√∂bb f√°jlban
```

---

### 5. Hibakezel√©s

**Mindig r√∂gz√≠tsd a metrik√°kat, m√©g hib√°k eset√©n is**:

```python
@contextmanager
def record_operation():
    start_time = time.time()
    status = "error"  # Alap√©rtelmezett hiba
    
    try:
        yield
        status = "success"  # Csak siker eset√©n √°ll√≠tsd be
    finally:
        # Mindig r√∂gz√≠tsd, m√©g kiv√©tel eset√©n is
        duration = time.time() - start_time
        operation_count.labels(status=status).inc()
        operation_duration.observe(duration)
```

---

## ÔøΩ Fejlett Megfigyelhet≈ës√©gi Funkci√≥k

A projekt a standard metrik√°k mellett fejlett megfigyelhet≈ës√©gi funkci√≥kat is tartalmaz, amelyek m√©lyebb betekint√©st ny√∫jtanak az agent m≈±k√∂d√©s√©be.

### 1. Prompt Lineage (Prompt Lesz√°rmaz√°s K√∂vet√©s)

**K√≥d Lok√°ci√≥**: `backend/observability/prompt_lineage.py`

**Mi ez?**: Minden LLM h√≠v√°shoz r√∂gz√≠ti a prompt hash-√©t, verzi√≥j√°t √©s metaadatait, lehet≈ëv√© t√©ve a prompt evol√∫ci√≥ k√∂vet√©s√©t √©s az LLM viselked√©s debug-ol√°s√°t.

**Implement√°ci√≥**:

```python
# F√°jl: backend/observability/prompt_lineage.py

@dataclass
class PromptLineage:
    """Prompt lesz√°rmaz√°si rekord LLM h√≠v√°sok k√∂vet√©s√©re."""
    prompt_hash: str           # SHA256 hash a teljes prompt sz√∂vegr≈ël
    request_id: str            # Egyedi k√©r√©s azonos√≠t√≥
    agent_execution_id: str    # Egyedi agent v√©grehajt√°s azonos√≠t√≥
    model_name: str            # Haszn√°lt LLM modell
    timestamp: str             # ISO timestamp
    prompt_version: Optional[str] = None  # Prompt template verzi√≥
    message_count: int = 0     # √úzenetek sz√°ma
    total_chars: int = 0       # √ñsszes karakter sz√°m

class PromptLineageTracker:
    """K√∂veti a prompt lesz√°rmaz√°st LLM h√≠v√°sokon kereszt√ºl."""
    
    def track_prompt(
        self,
        messages: List[BaseMessage],
        model_name: str,
        agent_execution_id: str,
        prompt_version: Optional[str] = None
    ) -> PromptLineage:
        """
        Prompt h√≠v√°s k√∂vet√©se.
        
        R√∂gz√≠ti:
        - Prompt hash (SHA256)
        - √úzenetek sz√°ma
        - Karakter sz√°m
        - Id≈ëb√©lyeg
        - Model n√©v
        """
        prompt_text = self._messages_to_text(messages)
        prompt_hash = self._hash_prompt(prompt_text)
        
        lineage = PromptLineage(
            prompt_hash=prompt_hash,
            request_id=get_request_id(),
            agent_execution_id=agent_execution_id,
            model_name=model_name,
            timestamp=datetime.utcnow().isoformat(),
            message_count=len(messages),
            total_chars=len(prompt_text)
        )
        
        self._lineage_records.append(lineage)
        return lineage
```

**Haszn√°lat a k√≥dban**:

```python
# F√°jl: backend/observability/llm_instrumentation.py

from observability.prompt_lineage import get_prompt_tracker

async def instrumented_llm_call(llm, messages, model, agent_execution_id):
    # Prompt lineage k√∂vet√©s
    if agent_execution_id:
        tracker = get_prompt_tracker()
        tracker.track_prompt(
            messages=messages,
            model_name=model,
            agent_execution_id=agent_execution_id
        )
    
    # LLM h√≠v√°s
    response = await llm.ainvoke(messages)
    return response
```

**Mit ny√∫jt?**:
- ‚úÖ Prompt verzi√≥ k√∂vet√©s id≈ëben
- ‚úÖ Azonos prompt ism√©tl≈ëd√©sek √©szlel√©se
- ‚úÖ LLM viselked√©s debug-ol√°s prompt alapj√°n
- ‚úÖ A/B tesztel√©s t√°mogat√°s k√ºl√∂nb√∂z≈ë prompt verzi√≥khoz

---

### 2. Agent Decision Trace (LangGraph State Snapshots)

**K√≥d Lok√°ci√≥**: `backend/observability/state_tracker.py`

**Mi ez?**: LangGraph √°llapot pillanatfelv√©telek r√∂gz√≠t√©se kritikus pontokon az agent v√©grehajt√°s sor√°n (v√©grehajt√°s el≈ëtt, minden node ut√°n, befejez√©skor).

**Implement√°ci√≥**:

```python
# F√°jl: backend/observability/state_tracker.py

@dataclass
class StateSnapshot:
    """LangGraph √°llapot pillanatfelv√©tel egy adott v√©grehajt√°si ponton."""
    snapshot_id: str           # Egyedi snapshot azonos√≠t√≥
    agent_execution_id: str    # Agent v√©grehajt√°s azonos√≠t√≥
    timestamp: str             # ISO timestamp
    snapshot_type: str         # before_execution, after_node, after_completion
    node_name: Optional[str]   # Node neve (ha alkalmazhat√≥)
    state_summary: Dict[str, Any]  # √ñsszefoglalt √°llapot (teljes prompt n√©lk√ºl)
    metadata: Dict[str, Any]   # Tov√°bbi metaadatok

class StateTracker:
    """LangGraph √°llapot pillanatfelv√©telek k√∂vet√©se agent d√∂nt√©sek nyomk√∂vet√©s√©hez."""
    
    def snapshot_before_execution(
        self,
        agent_execution_id: str,
        initial_state: Dict[str, Any]
    ) -> StateSnapshot:
        """√Ållapot r√∂gz√≠t√©se az agent v√©grehajt√°s kezdete el≈ëtt."""
        snapshot = self._create_snapshot(
            agent_execution_id=agent_execution_id,
            snapshot_type="before_execution",
            state=initial_state
        )
        logger.info(f"State snapshot (before): exec_id={agent_execution_id}")
        return snapshot
    
    def snapshot_after_node(
        self,
        agent_execution_id: str,
        node_name: str,
        state: Dict[str, Any]
    ) -> StateSnapshot:
        """√Ållapot r√∂gz√≠t√©se egy node v√©grehajt√°sa ut√°n."""
        snapshot = self._create_snapshot(
            agent_execution_id=agent_execution_id,
            snapshot_type="after_node",
            node_name=node_name,
            state=state
        )
        logger.info(f"State snapshot (after_node): node={node_name}")
        return snapshot
    
    def snapshot_after_completion(
        self,
        agent_execution_id: str,
        final_state: Dict[str, Any]
    ) -> StateSnapshot:
        """√Ållapot r√∂gz√≠t√©se az agent v√©grehajt√°s befejez√©se ut√°n."""
        snapshot = self._create_snapshot(
            agent_execution_id=agent_execution_id,
            snapshot_type="after_completion",
            state=final_state
        )
        logger.info(f"State snapshot (completion): exec_id={agent_execution_id}")
        return snapshot
```

**Haszn√°lat az agent k√≥dban**:

```python
# F√°jl: backend/services/agent.py

from observability.state_tracker import get_state_tracker

class AIAgent:
    async def run(self, user_id: str, message: str):
        agent_execution_id = f"exec_{uuid.uuid4().hex[:12]}"
        state_tracker = get_state_tracker()
        
        # Kezdeti √°llapot r√∂gz√≠t√©se
        initial_state = {"user_id": user_id, "message": message}
        state_tracker.snapshot_before_execution(
            agent_execution_id=agent_execution_id,
            initial_state=initial_state
        )
        
        # Graph v√©grehajt√°s - minden node ut√°n automatikus snapshot
        result = await self.graph.ainvoke(initial_state)
        
        # V√©gs≈ë √°llapot r√∂gz√≠t√©se
        state_tracker.snapshot_after_completion(
            agent_execution_id=agent_execution_id,
            final_state=result
        )
        
        return result
```

**Mit ny√∫jt?**:
- ‚úÖ Agent d√∂nt√©sek teljes nyomk√∂vet√©se
- ‚úÖ Node-ok k√∂z√∂tti √°llapotv√°ltoz√°sok l√°that√≥s√°ga
- ‚úÖ Debug t√°mogat√°s komplex multi-step folyamatokhoz
- ‚úÖ Replay k√©pess√©g - √°llapotok √∫jraj√°tsz√°sa

---

### 3. Token-szint≈± K√∂lts√©gfigyel√©s

**K√≥d Lok√°ci√≥**: `backend/observability/metrics.py` √©s `backend/observability/llm_instrumentation.py`

**Mi ez?**: R√©szletes token haszn√°lat √©s k√∂lts√©g k√∂vet√©s modell szerint, bemeneti/kimeneti tokenek k√ºl√∂n r√∂gz√≠t√©s√©vel.

**Implement√°ci√≥**:

```python
# F√°jl: backend/observability/metrics.py

# Token metrik√°k
llm_inference_token_input_total = Counter(
    name='llm_inference_token_input_total',
    documentation='√ñsszes bemeneti token LLM √°ltal feldolgozva',
    labelnames=['model'],
    registry=registry
)

llm_inference_token_output_total = Counter(
    name='llm_inference_token_output_total',
    documentation='√ñsszes kimeneti token LLM √°ltal gener√°lva',
    labelnames=['model'],
    registry=registry
)

llm_cost_total_usd = Counter(
    name='llm_cost_total_usd',
    documentation='Teljes k√∂lts√©g USD-ben LLM inferenci√°ra',
    labelnames=['model'],
    registry=registry
)

# K√∂lts√©g sz√°m√≠t√°s
def _estimate_llm_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float:
    """
    LLM k√∂lts√©g becsl√©se USD-ben token haszn√°lat alapj√°n.
    
    √Årk√©pz√©si t√°bl√°zat (USD per 1K token) - 2026. Jan:
    """
    PRICING = {
        "gpt-4o": (0.005, 0.015),           # input, output
        "gpt-4o-mini": (0.00015, 0.0006),   # $0.15/$0.60 per 1M token
        "gpt-4-turbo-preview": (0.01, 0.03),
        "gpt-3.5-turbo": (0.0015, 0.002),
    }
    
    input_price, output_price = PRICING.get(model, PRICING["gpt-4"])
    
    cost = (prompt_tokens / 1000.0 * input_price) + \
           (completion_tokens / 1000.0 * output_price)
    
    return cost
```

**Token √©s k√∂lts√©g r√∂gz√≠t√©s**:

```python
# F√°jl: backend/observability/llm_instrumentation.py

async def instrumented_llm_call(llm, messages, model: str):
    start_time = time.time()
    
    # LLM h√≠v√°s
    response = await llm.ainvoke(messages)
    duration = time.time() - start_time
    
    # Token haszn√°lat kinyer√©se
    prompt_tokens = response.usage_metadata.get('input_tokens', 0)
    completion_tokens = response.usage_metadata.get('output_tokens', 0)
    
    # Metrik√°k r√∂gz√≠t√©se
    llm_inference_count.labels(model=model).inc()
    llm_inference_token_input_total.labels(model=model).inc(prompt_tokens)
    llm_inference_token_output_total.labels(model=model).inc(completion_tokens)
    llm_inference_latency_seconds.labels(model=model).observe(duration)
    
    # K√∂lts√©g sz√°m√≠t√°s √©s r√∂gz√≠t√©s
    cost = _estimate_llm_cost(model, prompt_tokens, completion_tokens)
    llm_cost_total_usd.labels(model=model).inc(cost)
    
    logger.info(
        f"LLM call completed: model={model} "
        f"tokens_in={prompt_tokens} tokens_out={completion_tokens} "
        f"cost=${cost:.4f} duration={duration:.2f}s"
    )
    
    return response
```

**Grafana lek√©rdez√©sek k√∂lts√©g k√∂vet√©shez**:

```promql
# Teljes k√∂lts√©g modell szerint
sum by (model) (llm_cost_total_usd)

# K√∂lts√©g ar√°ny ($/√≥ra)
rate(llm_cost_total_usd[1h]) * 3600

# Token haszn√°lat ar√°ny
rate(llm_inference_token_input_total[5m])
rate(llm_inference_token_output_total[5m])

# √Åtlagos k√∂lts√©g h√≠v√°sonk√©nt
sum(increase(llm_cost_total_usd[5m])) / sum(increase(llm_inference_count[5m]))
```

**Mit ny√∫jt?**:
- ‚úÖ Val√≥s idej≈± k√∂lts√©g monitoring
- ‚úÖ Token haszn√°lat optimaliz√°l√°s
- ‚úÖ Budget riaszt√°sok be√°ll√≠t√°sa
- ‚úÖ Modell k√∂lts√©g √∂sszehasonl√≠t√°s

---

### 4. Model Fallback Path L√°t√°sa

**K√≥d Lok√°ci√≥**: `backend/observability/llm_instrumentation.py` √©s `backend/observability/metrics.py`

**Mi ez?**: Automatikus tartal√©k modell (fallback) haszn√°lat k√∂vet√©se, amikor az els≈ëdleges modell hib√°zik.

**Implement√°ci√≥**:

```python
# F√°jl: backend/observability/metrics.py

model_fallback_count = Counter(
    name='model_fallback_count',
    documentation='Model fallback el≈ëfordul√°sok teljes sz√°ma',
    labelnames=['from_model', 'to_model'],
    registry=registry
)

max_retries_exceeded_count = Counter(
    name='max_retries_exceeded_count',
    documentation='Maxim√°lis √∫jrapr√≥b√°lkoz√°sok t√∫ll√©p√©s√©nek sz√°ma',
    registry=registry
)
```

**Fallback logika**:

```python
# F√°jl: backend/observability/llm_instrumentation.py

async def instrumented_llm_call_with_fallback(
    primary_llm,
    fallback_llm,
    messages,
    primary_model: str,
    fallback_model: str,
    max_retries: int = 3
):
    """
    Instrument√°lt LLM h√≠v√°s automatikus fallback-kel m√°sodlagos modellre.
    
    Model fallback √∫tvonalakat k√∂vet a model_fallback_count metrik√°n kereszt√ºl.
    """
    request_id = get_request_id()
    
    # Pr√≥b√°ld az els≈ëdleges modellt el≈ësz√∂r
    for attempt in range(max_retries):
        try:
            logger.info(
                f"Els≈ëdleges modell pr√≥b√°lkoz√°s [model={primary_model}, "
                f"k√≠s√©rlet={attempt+1}/{max_retries}]"
            )
            response = await instrumented_llm_call(
                llm=primary_llm,
                messages=messages,
                model=primary_model
            )
            return response
            
        except Exception as e:
            logger.warning(
                f"Els≈ëdleges modell hib√°zott [model={primary_model}, "
                f"k√≠s√©rlet={attempt+1}/{max_retries}, hiba={type(e).__name__}]"
            )
            if attempt == max_retries - 1:
                # T√∫ll√©pte az √∫jrapr√≥b√°lkoz√°sokat, pr√≥b√°ld a fallback-et
                break
    
    # Fallback k√∂vet√©se metrik√°val
    model_fallback_count.labels(
        from_model=primary_model,
        to_model=fallback_model
    ).inc()
    
    logger.info(
        f"Fallback m√°sodlagos modellre [from={primary_model}, "
        f"to={fallback_model}, request_id={request_id}]"
    )
    
    # Pr√≥b√°ld a fallback modellt
    for attempt in range(max_retries):
        try:
            logger.info(
                f"Fallback modell pr√≥b√°lkoz√°s [model={fallback_model}, "
                f"k√≠s√©rlet={attempt+1}/{max_retries}]"
            )
            response = await instrumented_llm_call(
                llm=fallback_llm,
                messages=messages,
                model=fallback_model
            )
            return response
            
        except Exception as e:
            logger.error(
                f"Fallback modell is hib√°zott [model={fallback_model}, "
                f"k√≠s√©rlet={attempt+1}/{max_retries}]"
            )
            if attempt == max_retries - 1:
                # Mindk√©t modell sikertelen
                max_retries_exceeded_count.inc()
                raise
```

**Haszn√°lat az agent k√≥dban**:

```python
# F√°jl: backend/services/agent.py

from observability.llm_instrumentation import instrumented_llm_call_with_fallback

class AIAgent:
    def __init__(self):
        self.primary_llm = ChatOpenAI(model="gpt-4o")
        self.fallback_llm = ChatOpenAI(model="gpt-4o-mini")  # Olcs√≥bb fallback
    
    async def _agent_decide_node(self, state):
        messages = self._prepare_messages(state)
        
        # LLM h√≠v√°s fallback t√°mogat√°ssal
        response = await instrumented_llm_call_with_fallback(
            primary_llm=self.primary_llm,
            fallback_llm=self.fallback_llm,
            messages=messages,
            primary_model="gpt-4o",
            fallback_model="gpt-4o-mini",
            max_retries=3
        )
        
        return response
```

**Grafana lek√©rdez√©sek fallback k√∂vet√©shez**:

```promql
# Fallback esem√©nyek sz√°ma
sum by (from_model, to_model) (model_fallback_count)

# Fallback ar√°ny
rate(model_fallback_count[5m])

# Max √∫jrapr√≥b√°lkoz√°sok t√∫ll√©p√©se
sum(max_retries_exceeded_count)

# Fallback % az √∂sszes h√≠v√°shoz k√©pest
(sum(model_fallback_count) / sum(llm_inference_count)) * 100
```

**Mit ny√∫jt?**:
- ‚úÖ Model megb√≠zhat√≥s√°g monitoring
- ‚úÖ Fallback gyakoris√°g k√∂vet√©s
- ‚úÖ K√∂lts√©g optimaliz√°l√°s (dr√°ga modellr≈ël olcs√≥bbra v√°lt√°s)
- ‚úÖ Rendszer rugalmass√°g n√∂vel√©s

---

## üìÇ Fejlett Megfigyelhet≈ës√©gi F√°jlok √Åttekint√©se

| F√°jl | Funkci√≥ | Mit K√∂vet |
|------|---------|-----------|
| `backend/observability/prompt_lineage.py` | Prompt k√∂vet√©s | Prompt hash, verzi√≥, evol√∫ci√≥ |
| `backend/observability/state_tracker.py` | √Ållapot snapshots | LangGraph √°llapotok node-ok k√∂z√∂tt |
| `backend/observability/llm_instrumentation.py` | LLM wrapper | Token, k√∂lts√©g, fallback |
| `backend/observability/metrics.py` | Metrika defin√≠ci√≥k | √ñsszes Prometheus metrika |
| `backend/observability/correlation.py` | K√©r√©s ID k√∂vet√©s | Request ID propag√°ci√≥ |

---

## üìç Metrik√°k K√≥d Szint≈± Lok√°ci√≥i

Ez a szakasz r√©szletesen bemutatja, hogy az egyes metrika kateg√≥ri√°k pontosan hol r√∂gz√≠t≈ëdnek a k√≥db√°zisban.

### 1. ü§ñ Modellh√≠v√°s Metrik√°k

**Metrik√°k**: `llm_inference_count`, `llm_inference_latency_seconds`, `llm_inference_token_input_total`, `llm_inference_token_output_total`

#### Defin√≠ci√≥ Helye:
**F√°jl**: `backend/observability/metrics.py`

```python
# Sorok: 232-257
llm_inference_count = Counter(
    name='llm_inference_count',
    documentation='√ñsszes LLM inferencia h√≠v√°s',
    labelnames=['model'],
    registry=registry
)

llm_inference_latency_seconds = Histogram(
    name='llm_inference_latency_seconds',
    documentation='LLM inferencia h√≠v√°sok k√©sleltet√©se m√°sodpercben',
    labelnames=['model'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0],
    registry=registry
)

llm_inference_token_input_total = Counter(
    name='llm_inference_token_input_total',
    documentation='√ñsszes bemeneti token LLM √°ltal feldolgozva',
    labelnames=['model'],
    registry=registry
)

llm_inference_token_output_total = Counter(
    name='llm_inference_token_output_total',
    documentation='√ñsszes kimeneti token LLM √°ltal gener√°lva',
    labelnames=['model'],
    registry=registry
)
```

#### R√∂gz√≠t√©s Helye:
**F√°jl**: `backend/observability/llm_instrumentation.py`

```python
# Funkci√≥: instrumented_llm_call() - Sorok: 32-140
async def instrumented_llm_call(llm, messages, model: str):
    """LLM h√≠v√°s automatikus metrika gy≈±jt√©ssel."""
    
    start_time = time.time()
    
    # LLM h√≠v√°s
    response = await llm.ainvoke(messages)
    
    # Id≈ëtartam sz√°m√≠t√°sa
    duration = time.time() - start_time
    
    # Token haszn√°lat kinyer√©se
    prompt_tokens = response.usage_metadata.get('input_tokens', 0)
    completion_tokens = response.usage_metadata.get('output_tokens', 0)
    
    # ‚úÖ METRIK√ÅK R√ñGZ√çT√âSE ITT
    llm_inference_count.labels(model=model).inc()
    llm_inference_latency_seconds.labels(model=model).observe(duration)
    llm_inference_token_input_total.labels(model=model).inc(prompt_tokens)
    llm_inference_token_output_total.labels(model=model).inc(completion_tokens)
    
    return response
```

#### Haszn√°lat az Agent K√≥dban:
**F√°jlok**: 
- `backend/services/agent.py` (sorok: 506-513, 906-913)
- `backend/advanced_agents/routing/router.py`
- `backend/advanced_agents/planning/planner.py`

```python
# backend/services/agent.py - agent_decide node
response = await instrumented_llm_call(
    llm=self.llm,
    messages=messages,
    model="gpt-4o-mini",
    agent_execution_id=state.get("agent_execution_id")
)
# ‚Üë Ez automatikusan r√∂gz√≠ti az √∂sszes LLM metrik√°t
```

---

### 2. üîÑ Agent Workflow Metrik√°k

**Metrik√°k**: `agent_execution_count`, `agent_execution_latency_seconds`, `node_execution_latency_seconds`, `tool_invocation_count`

#### Defin√≠ci√≥ Helye:
**F√°jl**: `backend/observability/metrics.py`

```python
# Sorok: 265-295
agent_execution_count = Counter(
    name='agent_execution_count',
    documentation='Agent v√©grehajt√°sok teljes sz√°ma',
    registry=registry
)

agent_execution_latency_seconds = Histogram(
    name='agent_execution_latency_seconds',
    documentation='Agent v√©grehajt√°s k√©sleltet√©se m√°sodpercben',
    buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0, 120.0],
    registry=registry
)

node_execution_latency_seconds = Histogram(
    name='node_execution_latency_seconds',
    documentation='Egyedi node v√©grehajt√°s k√©sleltet√©se m√°sodpercben',
    labelnames=['node'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0],
    registry=registry
)

tool_invocation_count = Counter(
    name='tool_invocation_count',
    documentation='Eszk√∂z h√≠v√°sok teljes sz√°ma',
    labelnames=['tool'],
    registry=registry
)
```

#### R√∂gz√≠t√©s Helye - Agent Execution Count:
**F√°jl**: `backend/advanced_agents/routing/router.py`

```python
# Sorok: 44, 141
from observability.metrics import record_node_duration, agent_execution_count

async def route(self, state: AdvancedAgentState):
    with record_node_duration("router"):
        # Track agent execution count els≈ë router h√≠v√°sn√°l
        iteration_count = state.get("iteration_count", 0)
        if iteration_count == 0:
            # ‚úÖ AGENT EXECUTION METRIKA R√ñGZ√çT√âSE ITT
            agent_execution_count.inc()
```

#### R√∂gz√≠t√©s Helye - Node Execution:
**F√°jl**: `backend/observability/metrics.py`

```python
# Context manager: record_node_duration() - Sorok: 657-686
@contextmanager
def record_node_duration(node_name: str):
    """Node v√©grehajt√°si id≈ë r√∂gz√≠t√©se."""
    start_time = time.time()
    
    try:
        yield
    finally:
        duration = time.time() - start_time
        
        # ‚úÖ NODE METRIK√ÅK R√ñGZ√çT√âSE ITT
        agent_node_executions_total.labels(
            node=node_name,
            environment=get_environment()
        ).inc()
        
        node_execution_latency_seconds.labels(node=node_name).observe(duration)
```

**Haszn√°lat minden node-ban**:
```python
# backend/services/agent.py - minden node wrapper-rel
async def _agent_decide_node(self, state):
    with record_node_duration("agent_decide"):  # ‚úÖ Node metrik√°k itt
        # Node logika...
        pass

async def _tool_execution_node(self, state):
    with record_node_duration("tool_execution"):  # ‚úÖ Node metrik√°k itt
        # Node logika...
        pass
```

#### R√∂gz√≠t√©s Helye - Tool Invocation:
**F√°jl**: `backend/observability/metrics.py`

```python
# Context manager: record_tool_call() - Sorok: 695-732
@contextmanager
def record_tool_call(tool_name: str):
    """Eszk√∂z h√≠v√°s metrik√°k r√∂gz√≠t√©se."""
    start_time = time.time()
    
    try:
        yield
        # ‚úÖ TOOL METRIK√ÅK R√ñGZ√çT√âSE ITT (sikeres)
        tool_invocation_count.labels(tool=tool_name).inc()
    except Exception as e:
        # ‚úÖ TOOL METRIK√ÅK R√ñGZ√çT√âSE ITT (hiba)
        tool_invocation_count.labels(tool=tool_name).inc()
        raise
    finally:
        duration = time.time() - start_time
        agent_tool_duration_seconds.labels(tool=tool_name).observe(duration)
```

**Haszn√°lat az eszk√∂z k√≥dban**:
**F√°jl**: `backend/services/tools.py`

```python
# Sorok: 15, 46, 97
from observability.metrics import record_tool_call

class WeatherTool:
    async def execute(self, city: str):
        with record_tool_call("weather"):  # ‚úÖ Eszk√∂z metrik√°k itt
            result = await self.client.get_forecast(city=city)
            return result

class GeocodeTool:
    async def execute(self, address: str):
        with record_tool_call("geocode"):  # ‚úÖ Eszk√∂z metrik√°k itt
            result = await self.client.geocode(address=address)
            return result
```

---

### 3. ‚ö†Ô∏è Hiba √©s Fallback Metrik√°k

**Metrik√°k**: `agent_errors_total`, `model_fallback_count`, `max_retries_exceeded_count`

#### Defin√≠ci√≥ Helye:
**F√°jl**: `backend/observability/metrics.py`

```python
# Sorok: 166-175
agent_errors_total = Counter(
    name='agent_errors_total',
    documentation='Agent v√©grehajt√°s hib√°k teljes sz√°ma',
    labelnames=['error_type', 'node', 'environment'],
    registry=registry
)

# Sorok: 300-309
model_fallback_count = Counter(
    name='model_fallback_count',
    documentation='Model fallback el≈ëfordul√°sok teljes sz√°ma',
    labelnames=['from_model', 'to_model'],
    registry=registry
)

max_retries_exceeded_count = Counter(
    name='max_retries_exceeded_count',
    documentation='Maxim√°lis √∫jrapr√≥b√°lkoz√°sok t√∫ll√©p√©s√©nek sz√°ma',
    registry=registry
)
```

#### R√∂gz√≠t√©s Helye - Hiba Metrik√°k:
**F√°jl**: `backend/observability/metrics.py`

```python
# Funkci√≥: record_error() - Sorok: 742-768
def record_error(error_type: str, node: str = "unknown"):
    """
    Hiba el≈ëfordul√°s r√∂gz√≠t√©se.
    
    Hiba t√≠pusok:
        - llm_error: LLM API hib√°k, rate limit, stb.
        - tool_error: K√ºls≈ë eszk√∂z/API hib√°k
        - validation_error: Bemenet valid√°ci√≥s hib√°k
        - rag_error: RAG lek√©r√©si hib√°k
        - unknown: Nem oszt√°lyozott hib√°k
    """
    # ‚úÖ HIBA METRIKA R√ñGZ√çT√âSE ITT
    agent_errors_total.labels(
        error_type=error_type,
        node=node,
        environment=get_environment()
    ).inc()
```

**Haszn√°lat az agent k√≥dban**:
**F√°jl**: `backend/services/agent.py`

```python
# Sorok: 20, 512, 912
from observability.metrics import record_node_duration, record_error

async def _agent_decide_node(self, state):
    try:
        response = await instrumented_llm_call(...)
    except Exception as e:
        logger.error(f"LLM h√≠v√°s sikertelen: {e}")
        # ‚úÖ HIBA METRIKA R√ñGZ√çT√âSE ITT
        record_error(error_type="llm_error", node="agent_decide")
        raise
```

**F√°jl**: `backend/observability/llm_instrumentation.py`

```python
# Sorok: 22, 134, 199
from observability.metrics import record_error

async def instrumented_llm_call(llm, messages, model):
    try:
        response = await llm.ainvoke(messages)
    except Exception as e:
        # ‚úÖ HIBA METRIKA R√ñGZ√çT√âSE ITT
        record_error(error_type="llm_error", node="llm_call")
        raise
```

#### R√∂gz√≠t√©s Helye - Fallback Metrik√°k:
**F√°jl**: `backend/observability/llm_instrumentation.py`

```python
# Funkci√≥: instrumented_llm_call_with_fallback() - Sorok: 250-351
async def instrumented_llm_call_with_fallback(
    primary_llm, fallback_llm, messages,
    primary_model: str, fallback_model: str, max_retries: int = 3
):
    # Els≈ëdleges modell pr√≥b√°lkoz√°sok
    for attempt in range(max_retries):
        try:
            return await instrumented_llm_call(primary_llm, messages, primary_model)
        except Exception as e:
            if attempt == max_retries - 1:
                break
    
    # ‚úÖ FALLBACK METRIKA R√ñGZ√çT√âSE ITT
    model_fallback_count.labels(
        from_model=primary_model,
        to_model=fallback_model
    ).inc()
    
    # Fallback modell pr√≥b√°lkoz√°sok
    for attempt in range(max_retries):
        try:
            return await instrumented_llm_call(fallback_llm, messages, fallback_model)
        except Exception as e:
            if attempt == max_retries - 1:
                # ‚úÖ MAX RETRIES METRIKA R√ñGZ√çT√âSE ITT
                max_retries_exceeded_count.inc()
                raise
```

---

### 4. üí∞ K√∂lts√©g Metrik√°k

**Metrik√°k**: `llm_cost_total_usd`, `agent_llm_cost_usd_total`

#### Defin√≠ci√≥ Helye:
**F√°jl**: `backend/observability/metrics.py`

```python
# Sorok: 256-262
llm_cost_total_usd = Counter(
    name='llm_cost_total_usd',
    documentation='Teljes k√∂lts√©g USD-ben LLM inferenci√°ra',
    labelnames=['model'],
    registry=registry
)

# Sorok: 93-101 (r√©szletes v√°ltozat)
agent_llm_cost_usd_total = Counter(
    name='agent_llm_cost_usd_total',
    documentation='Becs√ºlt LLM k√∂lts√©gek USD-ben',
    labelnames=['model', 'tenant', 'environment'],
    registry=registry
)
```

#### K√∂lts√©g Sz√°m√≠t√°s Helye:
**F√°jl**: `backend/observability/metrics.py`

```python
# Funkci√≥: _estimate_llm_cost() - Sorok: 576-603
def _estimate_llm_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float:
    """
    LLM k√∂lts√©g becsl√©se USD-ben token haszn√°lat alapj√°n.
    
    √Årk√©pz√©si t√°bl√°zat (USD per 1K token) - 2026. Jan:
    """
    PRICING = {
        "gpt-4-turbo-preview": (0.01, 0.03),  # bemenet, kimenet
        "gpt-4": (0.03, 0.06),
        "gpt-4.1": (0.03, 0.06),
        "gpt-3.5-turbo": (0.0015, 0.002),
        "gpt-4o": (0.005, 0.015),
        "gpt-4o-mini": (0.00015, 0.0006),  # $0.15/$0.60 per 1M token
    }
    
    input_price, output_price = PRICING.get(model, PRICING["gpt-4"])
    
    # ‚úÖ K√ñLTS√âG SZ√ÅM√çT√ÅSA ITT
    cost = (prompt_tokens / 1000.0 * input_price) + \
           (completion_tokens / 1000.0 * output_price)
    
    return cost
```

#### R√∂gz√≠t√©s Helye:
**F√°jl**: `backend/observability/metrics.py`

```python
# Funkci√≥: record_llm_usage() - Sorok: 500-574
def record_llm_usage(
    model: str,
    prompt_tokens: int,
    completion_tokens: int,
    duration_seconds: float
):
    """LLM haszn√°lat metrik√°k r√∂gz√≠t√©se tokenekkel √©s becs√ºlt k√∂lts√©ggel."""
    
    # Token metrik√°k r√∂gz√≠t√©se
    agent_llm_tokens_total.labels(model=model, direction="prompt").inc(prompt_tokens)
    agent_llm_tokens_total.labels(model=model, direction="completion").inc(completion_tokens)
    
    # K√∂lts√©g sz√°m√≠t√°sa
    cost_usd = _estimate_llm_cost(model, prompt_tokens, completion_tokens)
    
    # ‚úÖ K√ñLTS√âG METRIK√ÅK R√ñGZ√çT√âSE ITT
    agent_llm_cost_usd_total.labels(model=model).inc(cost_usd)
    llm_cost_total_usd.labels(model=model).inc(cost_usd)
```

**Haszn√°lat az LLM wrapper-ben**:
**F√°jl**: `backend/observability/llm_instrumentation.py`

```python
# Sorok: 95-120
async def instrumented_llm_call(llm, messages, model):
    start_time = time.time()
    response = await llm.ainvoke(messages)
    duration = time.time() - start_time
    
    # Token haszn√°lat kinyer√©se
    prompt_tokens = response.usage_metadata.get('input_tokens', 0)
    completion_tokens = response.usage_metadata.get('output_tokens', 0)
    
    # ‚úÖ K√ñLTS√âG METRIK√ÅK R√ñGZ√çT√âSE ITT (record_llm_usage-en kereszt√ºl)
    record_llm_usage(
        model=model,
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        duration_seconds=duration
    )
```

---

### 5. üîç RAG (Retrieval-Augmented Generation) Metrik√°k

**Metrik√°k**: `rag_chunk_retrieval_count`, `rag_retrieved_chunk_relevance_score_avg`, `vector_db_query_latency_seconds`, `embedding_generation_count`

#### Defin√≠ci√≥ Helye:
**F√°jl**: `backend/observability/metrics.py`

```python
# Sorok: 177-215
agent_rag_retrievals_total = Counter(
    name='agent_rag_retrievals_total',
    documentation='RAG lek√©r√©sek teljes sz√°ma',
    labelnames=['status', 'environment'],
    registry=registry
)

agent_rag_chunks_retrieved = Histogram(
    name='agent_rag_chunks_retrieved',
    documentation='Lek√©rt chunk-ok sz√°ma RAG lek√©rdez√©senk√©nt',
    labelnames=['environment'],
    buckets=[0, 1, 2, 5, 10, 20, 50],
    registry=registry
)

agent_rag_duration_seconds = Histogram(
    name='agent_rag_duration_seconds',
    documentation='RAG lek√©r√©si k√©sleltet√©s m√°sodpercben',
    labelnames=['environment'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0],
    registry=registry
)

# Sorok: 314-335 (spec-kompatibilis v√°ltozatok)
rag_chunk_retrieval_count = Counter(
    name='rag_chunk_retrieval_count',
    documentation='RAG chunk lek√©r√©sek teljes sz√°ma',
    registry=registry
)

rag_retrieved_chunk_relevance_score_avg = Gauge(
    name='rag_retrieved_chunk_relevance_score_avg',
    documentation='Lek√©rt chunk-ok √°tlagos relevancia pontsz√°ma',
    registry=registry
)

vector_db_query_latency_seconds = Histogram(
    name='vector_db_query_latency_seconds',
    documentation='Vektor adatb√°zis lek√©rdez√©sek k√©sleltet√©se m√°sodpercben',
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0],
    registry=registry
)

embedding_generation_count = Counter(
    name='embedding_generation_count',
    documentation='Embedding gener√°l√°sok teljes sz√°ma',
    registry=registry
)
```

#### R√∂gz√≠t√©s Helye:
**F√°jl**: `backend/observability/metrics.py`

```python
# Context manager: record_rag_retrieval() - Sorok: 772-849
@contextmanager
def record_rag_retrieval(num_chunks: int = 0, relevance_scores: Optional[List[float]] = None):
    """
    RAG lek√©r√©si metrik√°k r√∂gz√≠t√©se.
    
    Haszn√°lat:
        with record_rag_retrieval(num_chunks=5, relevance_scores=[0.9, 0.85, 0.8]):
            chunks = await vector_db.search(query)
    """
    start_time = time.time()
    status = "success"
    
    try:
        yield
    except Exception as e:
        status = "error"
        raise
    finally:
        duration = time.time() - start_time
        
        # ‚úÖ RAG METRIK√ÅK R√ñGZ√çT√âSE ITT
        agent_rag_retrievals_total.labels(
            status=status,
            environment=get_environment()
        ).inc()
        
        rag_chunk_retrieval_count.inc()
        
        agent_rag_chunks_retrieved.labels(
            environment=get_environment()
        ).observe(num_chunks)
        
        agent_rag_duration_seconds.labels(
            environment=get_environment()
        ).observe(duration)
        
        vector_db_query_latency_seconds.observe(duration)
        
        # Relevancia pontsz√°m √°tlag
        if relevance_scores:
            avg_score = sum(relevance_scores) / len(relevance_scores)
            rag_retrieved_chunk_relevance_score_avg.set(avg_score)
```

**Potenci√°lis haszn√°lat RAG szolg√°ltat√°sban**:
```python
# P√©lda: backend/services/rag_service.py (ha l√©tezik)
from observability.metrics import record_rag_retrieval, embedding_generation_count

class RAGService:
    async def retrieve_chunks(self, query: str, top_k: int = 5):
        # Embedding gener√°l√°s
        embedding_generation_count.inc()
        query_embedding = await self.embedding_model.encode(query)
        
        # Vektor DB lek√©rdez√©s metrika r√∂gz√≠t√©ssel
        with record_rag_retrieval():
            results = await self.vector_db.search(
                query_embedding,
                top_k=top_k
            )
            
            # Relevancia pontsz√°mok kinyer√©se
            relevance_scores = [r['score'] for r in results]
            
        # Metrik√°k friss√≠t√©se relevancia pontokkal
        with record_rag_retrieval(
            num_chunks=len(results),
            relevance_scores=relevance_scores
        ):
            pass  # M√°r lek√©rt√ºk, csak r√∂gz√≠tj√ºk a metrik√°kat
        
        return results
```

---

## üìä Metrika Kateg√≥ri√°k √ñsszefoglal√°sa

| Kateg√≥ria | F≈ë Defin√≠ci√≥ | F≈ë R√∂gz√≠t√©s | Haszn√°lat |
|-----------|--------------|-------------|-----------|
| **Modellh√≠v√°s** | `metrics.py` (232-257) | `llm_instrumentation.py` (95-120) | `agent.py`, `router.py`, `planner.py` |
| **Agent Workflow** | `metrics.py` (265-295) | `metrics.py` (657-732), `router.py` (141) | Minden agent node |
| **Hiba/Fallback** | `metrics.py` (166-175, 300-309) | `metrics.py` (742-768), `llm_instrumentation.py` (250-351) | `agent.py`, hibakezel≈ë blokkok |
| **K√∂lts√©g** | `metrics.py` (256-262, 93-101) | `metrics.py` (500-603), `llm_instrumentation.py` (110-115) | `llm_instrumentation.py` |
| **RAG** | `metrics.py` (177-215, 314-335) | `metrics.py` (772-849) | RAG szolg√°ltat√°sok |

---

## ÔøΩüìö √ñsszefoglal√°s

### Python ‚Üí Prometheus ‚Üí Grafana Folyamat

1. **Python K√≥d**: Haszn√°ld a `prometheus_client`-et metrik√°k defini√°l√°s√°hoz √©s r√∂gz√≠t√©s√©hez
2. **V√©gpont Kiszolg√°l√°sa**: FastAPI szolg√°ltatja a metrik√°kat a `/metrics` c√≠men
3. **Prometheus Gy≈±jt√©s**: 15 m√°sodpercenk√©nt a Prometheus lek√©ri √©s t√°rolja a metrik√°kat
4. **Grafana Lek√©rdez√©s**: Dashboardok lek√©rdezik a Prometheus-t PromQL-lel
5. **Vizualiz√°ci√≥**: A felhaszn√°l√≥k val√≥s idej≈± diagramokat √©s statisztik√°kat l√°tnak

### Kulcsfontoss√°g√∫ Metrik√°k ebben a Projektben

| Metrika | T√≠pus | C√©l | LangGraph Integr√°ci√≥ |
|---------|-------|-----|----------------------|
| `llm_inference_count` | Counter | LLM h√≠v√°sok sz√°mol√°sa | `instrumented_llm_call()` wrapperben r√∂gz√≠tve |
| `llm_inference_latency_seconds` | Histogram | LLM v√°laszid≈ë m√©r√©se | Id≈ëz√≠t√©ssel r√∂gz√≠tve a wrapperben |
| `tool_invocation_count` | Counter | Eszk√∂z h√≠v√°sok sz√°mol√°sa | `record_tool_call()` context managerben r√∂gz√≠tve |
| `node_execution_latency_seconds` | Histogram | Node v√©grehajt√°si id≈ë m√©r√©se | `record_node_duration()` minden LangGraph node k√∂r√ºl |
| `agent_execution_count` | Counter | Agent fut√°sok sz√°mol√°sa | agent.run() h√≠v√°sonk√©nt egyszer r√∂gz√≠tve |
| `llm_cost_total_usd` | Counter | Kumulat√≠v k√∂lts√©g k√∂vet√©se | Token haszn√°latb√≥l sz√°m√≠tva az LLM wrapperben |

### Fontos F√°jlok

| F√°jl | C√©l |
|------|-----|
| `backend/observability/metrics.py` | Metrika defin√≠ci√≥k |
| `backend/observability/llm_instrumentation.py` | LLM h√≠v√°s wrapper metrik√°kkal |
| `backend/services/agent.py` | LangGraph node-ok `record_node_duration()`-nel |
| `backend/services/tools.py` | Eszk√∂z wrapperek `record_tool_call()`-lal |
| `backend/main.py` | FastAPI `/metrics` v√©gpont be√°ll√≠t√°s |
| `observability/prometheus.yml` | Prometheus gy≈±jt√©si konfigur√°ci√≥ |
| `observability/grafana/dashboards/*.json` | El≈ëre elk√©sz√≠tett Grafana dashboardok |

---

**√ötmutat√≥ V√©ge**

Tov√°bbi inform√°ci√≥k√©rt:
- [DOCKER_ARCHITECTURE.md](DOCKER_ARCHITECTURE.md) - Kont√©ner be√°ll√≠t√°s
- [MONITORING_TEST_PROMPTS.md](MONITORING_TEST_PROMPTS.md) - Teszt promptok metrik√°k gener√°l√°s√°hoz
- [docs/09_MONITORING_PROMPT.md](docs/09_MONITORING_PROMPT.md) - Eredeti monitoring specifik√°ci√≥
