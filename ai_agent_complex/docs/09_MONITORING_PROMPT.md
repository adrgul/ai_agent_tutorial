GitHub Copilot Prompt â€“ Deep Observability for LangGraph AI Agent
ğŸ¯ Objective
Extend the existing LangGraph-based AI agent application with deep observability, focusing strictly on:
Prompt lineage
Agent decision tracing (LangGraph state snapshots)
Token-level cost tracking
Model fallback path visibility
Prometheus-based metrics
Grafana dashboards
âš ï¸ Do NOT introduce any features beyond what is explicitly listed below.

ğŸ§  Context
Python backend
LangGraph used for agent workflows
LLM calls (multiple models with fallback)
Optional RAG (vector database)
Prometheus + Grafana already running via Docker
Backend, frontend, and monitoring stack are containerized

ğŸ§© Observability Scope (STRICT)
A) Deep Observability Capabilities (MANDATORY)
Implement ALL of the following:
Prompt lineage
Track prompt versions / hashes per LLM invocation
Associate each prompt with:
request_id
agent_execution_id
model_name
Agent decision trace
Capture LangGraph state snapshots:
before execution
after each node
after completion
Store snapshots in memory or structured logs (NO full prompt content)
Token-level cost tracking
Input tokens
Output tokens
Cost calculated per model using configured pricing
Model fallback path visibility
Track:
primary model attempt
fallback model usage
retry exhaustion
Count and expose fallback paths explicitly

ğŸ“Š Metrics to Implement (Prometheus)
1ï¸âƒ£ LLM Inference Metrics
Implement exactly the following metrics:
llm_inference_count{model}
llm_inference_latency_seconds{model}
llm_inference_token_input_total{model}
llm_inference_token_output_total{model}
llm_cost_total_usd{model}
All metrics must be emitted only via a centralized LLM wrapper.

2ï¸âƒ£ Agent Workflow Metrics (LangGraph)
Implement exactly the following metrics:
agent_execution_count
agent_execution_latency_seconds
node_execution_latency_seconds{node}
tool_invocation_count{tool}
rag_recall_rate (advanced â€“ derived relevance metric)
Node-level metrics must use LangGraph node names as labels.

3ï¸âƒ£ Error & Fallback Metrics
Implement exactly the following metrics:
agent_error_count
tool_failure_count{tool}
model_fallback_count{from_model,to_model}
max_retries_exceeded_count
Fallback metrics must reflect actual fallback paths, not retries.

4ï¸âƒ£ Cost Metrics (REAL-TIME â€“ REQUIRED)
Implement real-time cost accumulation:
llm_cost_total_usd{model="gpt-4o-mini"}
llm_cost_total_usd{model="gpt-4.1"}
Cost must be:
calculated per request
accumulated globally
derivable per model, per agent execution

5ï¸âƒ£ RAG Metrics
Implement exactly the following:
rag_chunk_retrieval_count
rag_retrieved_chunk_relevance_score_avg
vector_db_query_latency_seconds
embedding_generation_count
Relevance score must be aggregated as average per query.

ğŸ“¡ Instrumentation Rules
All metrics must:
be defined in a dedicated observability module
use Prometheus primitives only (Counter, Histogram, Gauge)
Avoid high-cardinality labels
Do NOT use user_id as a metric label
Use:
model
node
tool
agent_execution_id (logs only, NOT metrics)

ğŸŒ Metrics Exposure
Expose a single /metrics endpoint
Compatible with Prometheus scraping
No authentication
No business logic inside /metrics

ğŸ“Š Grafana Dashboards (JSON REQUIRED)
Create exactly 4 dashboards, exported as JSON.

ğŸ“˜ Dashboard 1: LLM Dashboard
Panels:
inference count
tokens_in / tokens_out
cost (USD)
latency p50 / p95 / p99
error rate

ğŸ” Dashboard 2: Agent Workflow Dashboard
Panels:
agent execution latency
node latency:
retriever
parser
router
tool invocation count
model fallback count

ğŸ“š Dashboard 3: RAG Dashboard
Panels:
retrieval latency
top-k relevance score
vector DB load
embedding generation count

ğŸ’° Dashboard 4: Cost Dashboard
Panels:
cost per model
cost per user (derived, not label-based)
cost per agent workflow
daily burn rate

ğŸ³ Docker Compatibility (MANDATORY)
Ensure:
Metrics are accessible from Prometheus container
No hardcoded hostnames
All ports configurable via env vars

ğŸš« Explicit Non-Goals (DO NOT IMPLEMENT)
âŒ No tracing frameworks (Jaeger, Tempo, OpenTelemetry)
âŒ No external SaaS observability tools
âŒ No logging of raw prompts or PII
âŒ No dashboard beyond the four specified
âŒ No metric beyond the listed ones

âœ… Final State Validation
After implementation, the system must allow:
Full visibility into:
prompt evolution
agent decision paths
model fallback behavior
real-time cost
Accurate Prometheus metrics
Ready-to-import Grafana dashboards
Zero impact on agent functional behavior

End of instructions.

