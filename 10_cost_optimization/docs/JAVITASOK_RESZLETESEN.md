# Jav√≠t√°sok R√©szletesen - Rossz verzi√≥ vs. J√≥ verzi√≥

**K√©sz√ºlt**: 2026. janu√°r 17.  
**C√©l**: R√©szletes technikai √∫tmutat√≥ a k√∂lts√©goptimaliz√°l√°si jav√≠t√°sokhoz

## üìã Tartalomjegyz√©k

1. [√Åttekint√©s](#√°ttekint√©s)
2. [Prompt Optimaliz√°l√°s](#1-prompt-optimaliz√°l√°s)
3. [Dinamikus Modell V√°laszt√°s](#2-dinamikus-modell-v√°laszt√°s)
4. [Gyors√≠t√≥t√°raz√°s Bekapcsol√°sa](#3-gyors√≠t√≥t√°raz√°s-bekapcsol√°sa)
5. [Munkafolyamat Optimaliz√°l√°s](#4-munkafolyamat-optimaliz√°l√°s)
6. [Token K√∂lts√©gvet√©s Korl√°toz√°sa](#5-token-k√∂lts√©gvet√©s-korl√°toz√°sa)
7. [√ñsszes√≠tett Hat√°s](#√∂sszes√≠tett-hat√°s)

---

## √Åttekint√©s

Ez a dokumentum **konkr√©t k√≥d p√©ld√°kkal** mutatja be, hogyan kell √°tdolgozni egy k√∂lts√©ges, ineffekt√≠v AI √°gensrendszert egy production-ready, k√∂lts√©goptimaliz√°lt v√°ltozatt√°.

### Mit tanulsz ebb≈ël a dokumentumb√≥l?

- ‚úÖ Hogyan √≠rj r√∂vid, hat√©kony promptokat
- ‚úÖ Hogyan v√°lassz olcs√≥bb modelleket egyszer≈± feladatokhoz
- ‚úÖ Hogyan implement√°lj node-szint≈± √©s embedding cache-t
- ‚úÖ Hogyan ker√ºld el a felesleges node-ok futtat√°s√°t
- ‚úÖ Hogyan korl√°tozd a token outputot

### Verzi√≥ √ñsszehasonl√≠t√°s

| Metrika | Rossz verzi√≥ | J√≥ verzi√≥ | Javul√°s |
|---------|--------------|-----------|---------|
| **K√∂lts√©g/egyszer≈± lek√©rdez√©s** | $0.025 | $0.0015 | **94% cs√∂kken√©s** |
| **LLM h√≠v√°sok/lek√©rdez√©s** | 4 (mindig) | 2-4 (adapt√≠v) | **50% √°tlag** |
| **Cache tal√°lati ar√°ny** | 0% | 40-60% | **√öj funkci√≥** |
| **p95 latency** | 4-6s | 1-2s | **70% gyorsabb** |
| **Input tokenek (√°tlag)** | 1200 | 180 | **85% cs√∂kken√©s** |
| **Output tokenek (√°tlag)** | 2500 | 250 | **90% cs√∂kken√©s** |

---

## 1. Prompt Optimaliz√°l√°s

### Probl√©ma: Hossz√∫, besz√©des promptok

A rossz verzi√≥ban a promptok t√∫l hossz√∫ak, felesleges magyar√°zatokkal, ami drasztikusan n√∂veli az input token k√∂lts√©geket.

### ‚ùå ROSSZ P√©lda - Triage Prompt

**F√°jl**: `prompts/triage_prompt.txt` (rossz verzi√≥)

```text
Hello! I'm your friendly AI assistant, and I'm here to help you with your question today!

Before I can provide you with the most helpful and accurate response, I need to carefully analyze and classify the type of question you're asking. This is a very important step in our conversation, and I want to make sure I do it right!

Let me explain the classification system I use:

1. SIMPLE questions are straightforward queries that don't require any additional context or deep analysis. These are questions like "What is the capital of France?" or "What's 2+2?" - questions that have clear, direct answers.

2. RETRIEVAL questions are those that require me to look up specific information from a knowledge base or context...

[... tov√°bbi 20 sor magyar√°zat ...]

Here's your question that I need to classify:
{user_input}

After careful consideration and analysis of your question, taking into account all the nuances and details, my classification is:

Classification:
```

**Token sz√°m**: ~350 input token  
**K√∂lts√©g**: 350 √ó $0.0001/1K = $0.000035 **csak a prompt√©rt**

### ‚úÖ J√ì P√©lda - Optimaliz√°lt Triage Prompt

**F√°jl**: `prompts/triage_prompt.txt` (j√≥ verzi√≥)

```text
Classify the query type. Output ONE word only.

Types:
- simple: factual, direct answer
- retrieval: requires looking up information
- complex: needs reasoning or analysis

Query: {user_input}

Classification:
```

**Token sz√°m**: ~45 input token  
**K√∂lts√©g**: 45 √ó $0.0001/1K = $0.000045  
**Megtakar√≠t√°s**: **87% kevesebb token**

### Implement√°ci√≥ - Prompt Bet√∂lt√©s

**F√°jl**: `app/nodes/triage_node.py`

```python
def _build_prompt(self, user_input: str) -> str:
    """
    Build minimal classification prompt.
    
    Cost optimization: Very short prompt to minimize input tokens.
    """
    # Load optimized prompt from file
    try:
        with open("prompts/triage_prompt.txt", "r") as f:
            template = f.read()
        return template.replace("{user_input}", user_input)
    except FileNotFoundError:
        # Fallback to inline prompt
        return f"""Classify query type. Output ONE word only.

Types:
- simple: factual, direct answer
- retrieval: requires looking up information
- complex: needs reasoning or analysis

Query: {user_input}

Classification:"""
```

### Reasoning Prompt Optimaliz√°l√°s

**‚ùå ROSSZ** (`prompts/reasoning_prompt.txt`):
```text
Greetings! I am your dedicated expert analyst, and I'm absolutely thrilled to help you work through this complex question today!

Let me introduce myself and explain my approach: I am a highly sophisticated analytical system...

[... 40 sor f√∂l√∂sleges bevezet√©s ...]
```

**‚úÖ J√ì** (`prompts/reasoning_prompt.txt`):
```text
Analyze this complex question using step-by-step reasoning.

Question: {user_input}
{context}
Analysis:
```

**Javul√°s**: 95% token cs√∂kken√©s a reasoning promptban

### Summary Prompt Optimaliz√°l√°s

**‚ùå ROSSZ** (`prompts/summary_prompt.txt`):
```text
Welcome! I'm your friendly AI assistant, and I'm here to help you get the perfect answer to your question!

Thank you so much for your patience as I've been working hard to gather all the information you need...

[... 30 sor k√∂sz√∂netnyilv√°n√≠t√°s √©s magyar√°zat ...]
```

**‚úÖ J√ì** (`prompts/summary_prompt.txt`):
```text
Provide a clear, concise answer.

Question: {user_input}
{retrieval_context}{reasoning_output}
Answer:
```

### üìä Prompt Optimaliz√°l√°s Hat√°sa

| Node | Rossz prompt (tokenek) | J√≥ prompt (tokenek) | Megtakar√≠t√°s |
|------|------------------------|---------------------|--------------|
| Triage | 350 | 45 | **87%** |
| Reasoning | 420 | 25 | **94%** |
| Summary | 380 | 30 | **92%** |
| **√Åtlag** | **383** | **33** | **91%** |

---

## 2. Dinamikus Modell V√°laszt√°s

### Probl√©ma: Minden feladatra a legdr√°g√°bb modell

A rossz verzi√≥ban minden node GPT-4-et haszn√°l, m√©g az egyszer≈± oszt√°lyoz√°shoz is.

### ‚ùå ROSSZ Implement√°ci√≥

**F√°jl**: `app/nodes/triage_node.py` (rossz verzi√≥)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector,
    cache: Cache
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    self.cache = cache
    # ‚ùå BAD PRACTICE: Using expensive model for simple classification task
    self.model_name = model_selector.get_model_name(ModelTier.EXPENSIVE)
```

**Modell**: GPT-4  
**K√∂lts√©g**: $0.01/1K input, $0.03/1K output  
**Probl√©ma**: 10-20x dr√°g√°bb mint kellene

### ‚úÖ J√ì Implement√°ci√≥ - Triage Node

**F√°jl**: `app/nodes/triage_node.py` (j√≥ verzi√≥)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector,
    cache: Cache
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    self.cache = cache
    # ‚úÖ GOOD PRACTICE: Use cheapest model for simple classification
    self.model_name = model_selector.get_model_name(ModelTier.CHEAP)
```

**Modell**: GPT-3.5-turbo  
**K√∂lts√©g**: $0.0001/1K input, $0.0002/1K output  
**Megtakar√≠t√°s**: **100x olcs√≥bb** input tokenekre

### ‚úÖ J√ì Implement√°ci√≥ - Summary Node

**F√°jl**: `app/nodes/summary_node.py` (j√≥ verzi√≥)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    # ‚úÖ GOOD PRACTICE: Use medium model for summary - balance quality and cost
    self.model_name = model_selector.get_model_name(ModelTier.MEDIUM)
```

**Modell**: GPT-4-turbo  
**K√∂lts√©g**: $0.001/1K input, $0.002/1K output  
**El≈ëny**: J√≥ min≈ës√©g, de 10x olcs√≥bb mint GPT-4

### ‚úÖ J√ì Implement√°ci√≥ - Reasoning Node

**F√°jl**: `app/nodes/reasoning_node.py` (v√°ltozatlan)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    # ‚úÖ Expensive model justified for complex reasoning
    self.model_name = model_selector.get_model_name(ModelTier.EXPENSIVE)
```

**Modell**: GPT-4  
**Indokl√°s**: Csak komplex lek√©rdez√©sekn√©l fut, ahol meg√©ri a magasabb min≈ës√©g

### ‚úÖ J√ì Implement√°ci√≥ - Retrieval Node

**F√°jl**: `app/nodes/retrieval_node.py` (j√≥ verzi√≥)

```python
def __init__(
    self,
    llm_client: LLMClient,
    cost_tracker: CostTracker,
    model_selector: ModelSelector,
    embedding_cache: Cache
):
    self.llm_client = llm_client
    self.cost_tracker = cost_tracker
    self.model_selector = model_selector
    self.embedding_cache = embedding_cache
    # ‚úÖ GOOD PRACTICE: Use cheap model for retrieval/embedding tasks
    self.model_name = model_selector.get_model_name(ModelTier.CHEAP)
```

### Model Tier Defin√≠ci√≥k

**F√°jl**: `app/llm/models.py`

```python
from enum import Enum

class ModelTier(str, Enum):
    """Model pricing tiers for cost optimization."""
    CHEAP = "cheap"      # gpt-3.5-turbo: $0.0001/$0.0002
    MEDIUM = "medium"    # gpt-4-turbo: $0.001/$0.002
    EXPENSIVE = "expensive"  # gpt-4: $0.01/$0.03
```

### üìä Modell V√°laszt√°s Hat√°sa

Egyszer≈± lek√©rdez√©s p√©lda: "Mi az 2+2?"

| Verzi√≥ | Triage | Retrieval | Reasoning | Summary | √ñssz K√∂lts√©g |
|--------|--------|-----------|-----------|---------|--------------|
| **Rossz** | GPT-4 | GPT-4 | GPT-4 | GPT-4 | **$0.025** |
| **J√≥** | GPT-3.5 | Kimarad | Kimarad | GPT-4-turbo | **$0.0015** |
| **Javul√°s** | 100x olcs√≥bb | - | - | 10x olcs√≥bb | **94% megtakar√≠t√°s** |

---

## 3. Gyors√≠t√≥t√°raz√°s Bekapcsol√°sa

### Probl√©ma: Gyors√≠t√≥t√°r kikapcsolva

A rossz verzi√≥ban a cache logika megvan, de sz√°nd√©kosan ki van kapcsolva minden node-ban.

### ‚ùå ROSSZ Implement√°ci√≥ - Triage Cache

**F√°jl**: `app/nodes/triage_node.py` (rossz verzi√≥)

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute triage node."""
    logger.info(f"Executing {self.NODE_NAME} node")
    
    async with async_timer() as timer_ctx:
        # Check cache first
        cache_key = generate_cache_key(self.NODE_NAME, state["user_input"])
        
        # ‚ùå BAD PRACTICE: Caching disabled - every request hits the LLM
        cache_lookup_start = time.time()
        cached_result = None  # Force cache miss
        cache_lookup_time = time.time() - cache_lookup_start
        
        if cached_result is not None:
            # This never executes...
            logger.info(f"Cache hit for {self.NODE_NAME}")
            # ...
        else:
            # Cache miss - call LLM
            logger.info(f"Cache miss for {self.NODE_NAME}")
            # ...
            response = await self.llm_client.complete(...)
            
            # ‚ùå BAD PRACTICE: Caching disabled - don't save results
            # await self.cache.set(cache_key, classification)
```

**Probl√©ma**: 
- `cached_result = None` - mindig cache miss
- `await self.cache.set(...)` - ki van kommentezve
- Minden azonos lek√©rdez√©s √∫jra h√≠vja az LLM-et

### ‚úÖ J√ì Implement√°ci√≥ - Triage Cache Bekapcsolva

**F√°jl**: `app/nodes/triage_node.py` (j√≥ verzi√≥)

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute triage node."""
    logger.info(f"Executing {self.NODE_NAME} node")
    
    async with async_timer() as timer_ctx:
        # Check cache first
        cache_key = generate_cache_key(self.NODE_NAME, state["user_input"])
        
        # ‚úÖ GOOD PRACTICE: Enable node-level caching for triage
        cache_lookup_start = time.time()
        cached_result = await self.cache.get(cache_key)  # ‚Üê Val√≥di cache lookup
        cache_lookup_time = time.time() - cache_lookup_start
        
        if cached_result is not None:
            # Cache hit - skip LLM call entirely!
            logger.info(f"Cache hit for {self.NODE_NAME}")
            metrics.record_cache_lookup(
                self.CACHE_NAME,
                self.NODE_NAME,
                hit=True,
                latency=cache_lookup_time
            )
            
            classification = cached_result
        else:
            # Cache miss - call LLM
            logger.info(f"Cache miss for {self.NODE_NAME}")
            metrics.record_cache_lookup(
                self.CACHE_NAME,
                self.NODE_NAME,
                hit=False,
                latency=cache_lookup_time
            )
            
            # Load prompt and call LLM
            prompt = self._build_prompt(state["user_input"])
            response = await self.llm_client.complete(
                prompt=prompt,
                model=self.model_name,
                max_tokens=10,
                temperature=0.0
            )
            
            classification = response.content.strip().lower()
            # Normalize...
            
            # Track cost
            self.cost_tracker.track_usage(...)
            metrics.record_llm_call(...)
            
            # ‚úÖ GOOD PRACTICE: Cache triage results for repeated queries
            await self.cache.set(cache_key, classification)  # ‚Üê Ment√©s cache-be
```

**V√°ltoztat√°sok**:
1. `cached_result = await self.cache.get(cache_key)` - val√≥di lookup
2. `await self.cache.set(cache_key, classification)` - ment√©s enged√©lyezve
3. Cache hit eset√©n: **0 LLM h√≠v√°s = 0 k√∂lts√©g**

### ‚úÖ J√ì Implement√°ci√≥ - Embedding Cache

**F√°jl**: `app/nodes/retrieval_node.py` (j√≥ verzi√≥)

```python
async def _get_embedding(self, text: str) -> str:
    """
    Get embedding for text (simulated with caching).
    
    In production, this would call an embedding model.
    Cache prevents recomputing embeddings for the same text.
    """
    cache_key = generate_cache_key(self.CACHE_NAME, text)
    
    # ‚úÖ GOOD PRACTICE: Enable embedding cache to avoid recomputation
    cache_lookup_start = time.time()
    cached_embedding = await self.embedding_cache.get(cache_key)  # ‚Üê Lookup
    cache_lookup_time = time.time() - cache_lookup_start
    
    if cached_embedding is not None:
        logger.info(f"Embedding cache hit")
        metrics.record_cache_lookup(
            self.CACHE_NAME,
            self.NODE_NAME,
            hit=True,
            latency=cache_lookup_time
        )
        return cached_embedding
    
    # Cache miss - compute embedding (simulated)
    logger.info(f"Embedding cache miss")
    metrics.record_cache_lookup(
        self.CACHE_NAME,
        self.NODE_NAME,
        hit=False,
        latency=cache_lookup_time
    )
    
    # Simulate embedding as deterministic hash
    embedding = hashlib.sha256(text.encode()).hexdigest()
    
    # ‚úÖ GOOD PRACTICE: Cache embeddings for reuse
    await self.embedding_cache.set(cache_key, embedding)  # ‚Üê Ment√©s
    
    return embedding
```

### Cache Kulcs Gener√°l√°s

**F√°jl**: `app/cache/keys.py`

```python
import hashlib

def generate_cache_key(prefix: str, content: str) -> str:
    """
    Generate deterministic cache key.
    
    Args:
        prefix: Cache namespace (e.g., "triage", "embedding")
        content: Content to hash (e.g., user input)
    
    Returns:
        Deterministic cache key
    """
    content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
    return f"{prefix}:{content_hash}"
```

**P√©lda**:
- Input: `generate_cache_key("triage", "What is Docker?")`
- Output: `"triage:a3f5c8b2e9d1f4a7"`

### üìä Cache Hat√°sa

P√©lda: Ugyanaz a lek√©rdez√©s 20x (benchmark mode)

| Fut√°s | Cache √Ållapot | LLM H√≠v√°s | K√∂lts√©g | Latency |
|-------|---------------|-----------|---------|---------|
| 1. | Miss | ‚úÖ Igen | $0.0015 | 1.2s |
| 2-20. | Hit | ‚ùå Nem | $0.0000 | 0.05s |
| **√ñssz** | 5% miss, 95% hit | 1 h√≠v√°s | **$0.0015** | ~0.1s √°tlag |

**Rossz verzi√≥ ugyanerre**: 20 √ó $0.025 = **$0.50** (333x dr√°g√°bb!)

---

## 4. Munkafolyamat Optimaliz√°l√°s

### Probl√©ma: Minden node fut minden lek√©rdez√©sn√©l

A rossz verzi√≥ban a routing logika ignor√°lja a triage eredm√©nyt √©s minden node-ot futtat.

### ‚ùå ROSSZ Implement√°ci√≥ - Agent Graph

**F√°jl**: `app/graph/agent_graph.py` (rossz verzi√≥)

```python
def route_after_triage(state: AgentState) -> Literal["retrieval", "reasoning", "summary"]:
    """
    ‚ùå BAD PRACTICE: Ignoring classification - always go to retrieval.
    This ensures ALL nodes run for EVERY request, regardless of actual need.
    """
    classification = state.get("classification")
    logger.info(f"Routing decision (ignored): {classification} - ALWAYS routing to retrieval")
    
    # ‚ùå BAD PRACTICE: Always route to retrieval to ensure all nodes execute
    return "retrieval"

workflow.add_conditional_edges(
    "triage",
    route_after_triage,
    {
        "retrieval": "retrieval",
        "reasoning": "retrieval",  # ‚ùå BAD PRACTICE: Changed to always go to retrieval
        "summary": "retrieval"     # ‚ùå BAD PRACTICE: Changed to always go to retrieval
    }
)

# ‚ùå BAD PRACTICE: Chain all nodes together - retrieval ‚Üí reasoning ‚Üí summary
# This ensures EVERY node runs for EVERY request
workflow.add_edge("retrieval", "reasoning")
workflow.add_edge("reasoning", "summary")
```

**Probl√©ma**:
- "What is 2+2?" ‚Üí triage, retrieval, reasoning, summary (4 node)
- "Hello" ‚Üí triage, retrieval, reasoning, summary (4 node)
- √ñsszes lek√©rdez√©s **mindig 4 node-ot** futtat

### ‚úÖ J√ì Implement√°ci√≥ - Intelligens Routing

**F√°jl**: `app/graph/agent_graph.py` (j√≥ verzi√≥)

```python
def route_after_triage(state: AgentState) -> Literal["retrieval", "reasoning", "summary"]:
    """
    ‚úÖ GOOD PRACTICE: Intelligent routing based on classification.
    
    This workflow optimization dramatically reduces costs:
    - simple: skip retrieval and reasoning, go straight to summary
    - retrieval: do retrieval, skip reasoning, then summary
    - complex: do retrieval and reasoning, then summary
    
    Graph-level caching opportunity:
    LangGraph supports graph-level persistence/checkpointing which could
    cache entire workflow executions. This would be configured via
    MemorySaver or SqliteSaver when compiling the graph.
    Example: app = workflow.compile(checkpointer=MemorySaver())
    """
    classification = state.get("classification")
    logger.info(f"Routing based on classification: {classification}")
    
    # ‚úÖ GOOD PRACTICE: Route intelligently to skip unnecessary nodes
    if classification == "simple":
        # Simple queries: skip all intermediate steps
        return "summary"
    elif classification == "retrieval":
        # Retrieval queries: get context, then summarize
        return "retrieval"
    else:  # complex
        # Complex queries: full pipeline with retrieval and reasoning
        return "retrieval"

workflow.add_conditional_edges(
    "triage",
    route_after_triage,
    {
        "retrieval": "retrieval",
        "reasoning": "retrieval",
        "summary": "summary"  # ‚úÖ Direct path for simple queries
    }
)

# ‚úÖ GOOD PRACTICE: Conditional routing after retrieval
def route_after_retrieval(state: AgentState) -> Literal["reasoning", "summary"]:
    """
    Route to reasoning only for complex queries, otherwise summarize.
    """
    classification = state.get("classification")
    if classification == "complex":
        return "reasoning"
    return "summary"

workflow.add_conditional_edges(
    "retrieval",
    route_after_retrieval,
    {
        "reasoning": "reasoning",
        "summary": "summary"  # ‚úÖ Skip reasoning for retrieval-only queries
    }
)

# Reasoning always goes to summary
workflow.add_edge("reasoning", "summary")
```

### ‚úÖ J√ì Implement√°ci√≥ - Node-szint≈± Early Exit

**F√°jl**: `app/nodes/reasoning_node.py` (j√≥ verzi√≥)

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute reasoning node."""
    logger.info(f"Executing {self.NODE_NAME} node")
    
    # ‚úÖ GOOD PRACTICE: Only run expensive reasoning for complex queries
    if state.get("classification") != "complex":
        logger.info("Skipping reasoning - not a complex query")
        return {
            "nodes_executed": state.get("nodes_executed", []) + [f"{self.NODE_NAME}_skipped"],
        }
    
    # Continue with expensive reasoning...
    async with async_timer() as timer_ctx:
        prompt = self._build_prompt(state["user_input"], state.get("retrieval_context"))
        
        response = await self.llm_client.complete(
            prompt=prompt,
            model=self.model_name,
            max_tokens=1000,
            temperature=0.3
        )
        # ... rest of implementation
```

**F√°jl**: `app/nodes/retrieval_node.py` (j√≥ verzi√≥)

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute retrieval node."""
    logger.info(f"Executing {self.NODE_NAME} node")
    
    # ‚úÖ GOOD PRACTICE: Only run retrieval when classification indicates it's needed
    if state.get("classification") not in ["retrieval", "complex"]:
        logger.info("Skipping retrieval - not needed for this query type")
        return {
            "nodes_executed": state.get("nodes_executed", []) + [f"{self.NODE_NAME}_skipped"],
        }
    
    # Continue with retrieval...
    async with async_timer() as timer_ctx:
        query_embedding = await self._get_embedding(state["user_input"])
        docs = await self._retrieve_documents(state["user_input"], query_embedding)
        # ... rest of implementation
```

### üìä Routing Hat√°sa

| Lek√©rdez√©s T√≠pus | Rossz Verzi√≥ | J√≥ Verzi√≥ | Node Megtakar√≠t√°s |
|------------------|--------------|-----------|-------------------|
| "What is 2+2?" | triage ‚Üí retrieval ‚Üí reasoning ‚Üí summary (4) | triage ‚Üí summary (2) | **50%** |
| "Find Docker docs" | triage ‚Üí retrieval ‚Üí reasoning ‚Üí summary (4) | triage ‚Üí retrieval ‚Üí summary (3) | **25%** |
| "Design distributed system" | triage ‚Üí retrieval ‚Üí reasoning ‚Üí summary (4) | triage ‚Üí retrieval ‚Üí reasoning ‚Üí summary (4) | **0%** (sz√ºks√©ges) |
| **√Åtlag** | **4 node/lek√©rdez√©s** | **2.5 node/lek√©rdez√©s** | **~40%** |

---

## 5. Token K√∂lts√©gvet√©s Korl√°toz√°sa

### Probl√©ma: T√∫l magas max_tokens √©rt√©kek

A rossz verzi√≥ban minden node feleslegesen magas `max_tokens` limitet haszn√°l.

### ‚ùå ROSSZ Implement√°ci√≥ - Pazarl√≥ Token Limitek

**F√°jl**: `app/nodes/triage_node.py` (rossz verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=2000,  # ‚ùå Wastefully high for one-word answer
    temperature=0.0
)
```

**Probl√©ma**: Csak egy sz√≥t v√°runk ("simple", "retrieval", "complex"), de 2000 tokent enged√©lyez√ºnk

**F√°jl**: `app/nodes/reasoning_node.py` (rossz verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=3000,  # ‚ùå Wastefully high
    temperature=0.3
)
```

**Probl√©ma**: 3000 token = ~2250 sz√≥, sokkal t√∂bb mint kellene

**F√°jl**: `app/nodes/summary_node.py` (rossz verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=2000,  # ‚ùå Wastefully high for summary
    temperature=0.5
)
```

**Probl√©ma**: Az √∂sszefoglal√≥ r√∂vid kell legyen, 2000 token felesleges

### ‚úÖ J√ì Implement√°ci√≥ - Szigor√∫ Token Limitek

**F√°jl**: `app/nodes/triage_node.py` (j√≥ verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=10,  # ‚úÖ Only need one word
    temperature=0.0  # Deterministic
)
```

**Indokl√°s**: 
- Kimenet: "simple" (1 token), "retrieval" (1 token), "complex" (1 token)
- 10 token: biztons√°gos marg√≥
- **200x kevesebb** mint a rossz verzi√≥

**F√°jl**: `app/nodes/reasoning_node.py` (j√≥ verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=1000,  # ‚úÖ Sufficient for most complex queries
    temperature=0.3  # Lower for more focused reasoning
)
```

**Indokl√°s**:
- 1000 token = ~750 sz√≥
- El√©g a legt√∂bb komplex elemz√©shez
- **3x kevesebb** mint a rossz verzi√≥

**F√°jl**: `app/nodes/summary_node.py` (j√≥ verzi√≥)

```python
response = await self.llm_client.complete(
    prompt=prompt,
    model=self.model_name,
    max_tokens=500,  # ‚úÖ Enough for quality summary
    temperature=0.5  # Balanced creativity
)
```

**Indokl√°s**:
- 500 token = ~375 sz√≥
- Elegend≈ë egy j√≥ √∂sszefoglal√≥hoz
- **4x kevesebb** mint a rossz verzi√≥
- K√©nyszer√≠t t√∂m√∂r v√°laszokra

### üìä Token Limit Hat√°sa

GPT-4 output tokenek √°raz√°sa: **$0.03/1K**

| Node | Rossz max_tokens | J√≥ max_tokens | Megtakar√≠t√°s | K√∂lts√©g cs√∂kken√©s |
|------|------------------|---------------|--------------|-------------------|
| Triage | 2000 | 10 | **99.5%** | $0.06 ‚Üí $0.0003 |
| Reasoning | 3000 | 1000 | **66%** | $0.09 ‚Üí $0.03 |
| Summary | 2000 | 500 | **75%** | $0.06 ‚Üí $0.015 |

**P√©lda sz√°m√≠t√°s** (complex lek√©rdez√©s, mind a 3 node fut):
- Rossz verzi√≥: 2000 + 3000 + 2000 = 7000 max tokens ‚Üí **$0.21** potenci√°lis k√∂lts√©g
- J√≥ verzi√≥: 10 + 1000 + 500 = 1510 max tokens ‚Üí **$0.045** potenci√°lis k√∂lts√©g
- **Megtakar√≠t√°s: 78%**

### A max_tokens Fontoss√°ga

1. **K√∂lts√©g kontroll**: Output tokenek gyakran dr√°g√°bbak mint input
2. **Latency kontroll**: Kevesebb token = gyorsabb gener√°l√°s
3. **Min≈ës√©g kontroll**: K√©nyszer√≠t t√∂m√∂rs√©gre, jobb v√°laszokat eredm√©nyez
4. **Kisz√°m√≠that√≥s√°g**: Fix fels≈ë limit a k√∂lts√©gekre

---

## 6. Gyors√≠t√≥t√°r Technik√°k - Programoz√≥i √ötmutat√≥

### √Åttekint√©s

Ez az alkalmaz√°s **h√°rom k√ºl√∂nb√∂z≈ë szint≈±** gyors√≠t√≥t√°raz√°si strat√©gi√°t haszn√°l a k√∂lts√©gek cs√∂kkent√©s√©re √©s a teljes√≠tm√©ny jav√≠t√°s√°ra:

1. **Node-szint≈± cache** - Node eredm√©nyek t√°rol√°sa
2. **Embedding cache** - Sz√°m√≠tott embeddingek√© t√°rol√°sa  
3. **Graph-szint≈± cache** - Teljes workflow √°llapot ment√©se (megeml√≠tve, nem implement√°lva)

### 6.1 Node-szint≈± Cache (Triage Cache)

#### Mi az?

A node-szint≈± cache **egy adott node kimenet√©t** t√°rolja adott inputhoz. Ha ugyanaz az input √∫jra meg√©rkezik, a node nem fut le, hanem a cached eredm√©nyt adja vissza.

#### Implement√°ci√≥

**F√°jl**: `app/cache/memory_cache.py`

```python
from typing import Optional
import time

class MemoryCache:
    """
    In-memory cache implementation with TTL support.
    
    Thread-safe for async operations.
    """
    
    def __init__(self, ttl_seconds: int = 3600, max_size: int = 1000):
        """
        Args:
            ttl_seconds: Time-to-live in seconds (default: 1 hour)
            max_size: Maximum number of cached items
        """
        self._cache = {}  # {key: (value, timestamp)}
        self._ttl = ttl_seconds
        self._max_size = max_size
    
    async def get(self, key: str) -> Optional[str]:
        """Get value from cache if not expired."""
        if key not in self._cache:
            return None
        
        value, timestamp = self._cache[key]
        
        # Check TTL
        if time.time() - timestamp > self._ttl:
            # Expired - remove and return None
            del self._cache[key]
            return None
        
        return value
    
    async def set(self, key: str, value: str) -> None:
        """Set value in cache with current timestamp."""
        # Evict oldest if at max size
        if len(self._cache) >= self._max_size:
            oldest_key = min(self._cache.keys(), 
                           key=lambda k: self._cache[k][1])
            del self._cache[oldest_key]
        
        self._cache[key] = (value, time.time())
    
    async def delete(self, key: str) -> None:
        """Remove key from cache."""
        if key in self._cache:
            del self._cache[key]
    
    async def clear(self) -> None:
        """Clear all cached items."""
        self._cache.clear()
```

#### Haszn√°lat a Triage Node-ban

**F√°jl**: `app/nodes/triage_node.py`

```python
async def execute(self, state: AgentState) -> Dict:
    """Execute triage node."""
    
    # 1. Generate cache key from input
    cache_key = generate_cache_key(self.NODE_NAME, state["user_input"])
    
    # 2. Check cache first
    cached_result = await self.cache.get(cache_key)
    
    if cached_result is not None:
        # 3. Cache HIT - return immediately, NO LLM call
        logger.info(f"Cache hit for {self.NODE_NAME}")
        metrics.record_cache_lookup(
            self.CACHE_NAME,
            self.NODE_NAME,
            hit=True,
            latency=cache_lookup_time
        )
        classification = cached_result
    else:
        # 4. Cache MISS - call LLM
        logger.info(f"Cache miss for {self.NODE_NAME}")
        
        # Call expensive LLM
        response = await self.llm_client.complete(...)
        classification = response.content.strip().lower()
        
        # 5. Save result to cache for next time
        await self.cache.set(cache_key, classification)
    
    return {"classification": classification, ...}
```

#### Cache Key Gener√°l√°s

**F√°jl**: `app/cache/keys.py`

```python
import hashlib

def generate_cache_key(prefix: str, content: str) -> str:
    """
    Generate deterministic cache key.
    
    Args:
        prefix: Namespace (e.g., "triage", "embedding")
        content: Content to hash (e.g., user input)
    
    Returns:
        Cache key format: "prefix:hash_of_content"
    
    Example:
        generate_cache_key("triage", "What is Docker?")
        ‚Üí "triage:a3f5c8b2e9d1f4a7"
    """
    # Use SHA-256 hash of content
    content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
    
    # Combine prefix and hash
    return f"{prefix}:{content_hash}"
```

**Mi√©rt fontos a determinisztikus hash?**
- Ugyanaz az input ‚Üí mindig ugyanaz a cache key
- K√ºl√∂nb√∂z≈ë inputok ‚Üí garant√°ltan k√ºl√∂nb√∂z≈ë kulcsok
- Nincs √ºtk√∂z√©s (collision)

#### Konfigur√°ci√≥

**F√°jl**: `app/config.py`

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # Cache settings
    cache_ttl_seconds: int = 3600  # 1 hour
    cache_max_size: int = 1000     # 1000 items max
    
    class Config:
        env_file = ".env"
```

**.env f√°jl**:
```bash
# Cache configuration
CACHE_TTL_SECONDS=3600    # 1 √≥ra
CACHE_MAX_SIZE=1000       # Max 1000 elem
```

#### Mikor haszn√°ld?

‚úÖ **J√≥ esetek**:
- Determinisztikus m≈±veletek (ugyanaz az input ‚Üí ugyanaz az output)
- Gyakran ism√©tl≈ëd≈ë lek√©rdez√©sek
- Dr√°ga LLM h√≠v√°sok (classification, extraction)
- R√∂vid TTL-lel (1-24 √≥ra)

‚ùå **Rossz esetek**:
- Nem-determinisztikus m≈±veletek (creative writing)
- Egyedi, soha nem ism√©tl≈ëd≈ë lek√©rdez√©sek
- √Ålland√≥an v√°ltoz√≥ adatok (real-time API-k)
- Nagyon hossz√∫ v√°laszok (mem√≥ria pazarl√°s)

#### Hat√°s

| Metrika | Cache Miss | Cache Hit | Javul√°s |
|---------|-----------|-----------|---------|
| **LLM h√≠v√°s** | 1 | 0 | **100% megtakar√≠t√°s** |
| **K√∂lts√©g** | $0.0000047 | $0.00 | **100% megtakar√≠t√°s** |
| **Latency** | ~1000ms | ~5ms | **200x gyorsabb** |
| **Throughput** | ~1 req/s | ~200 req/s | **200x nagyobb** |

---

### 6.2 Embedding Cache

#### Mi az?

Az embedding cache **sz√°m√≠tott embedding vektorokat** t√°rol. Mivel az embedding sz√°m√≠t√°s dr√°ga (API h√≠v√°s vagy GPU sz√°m√≠t√°s), a cache elker√ºli az √∫jrasz√°m√≠t√°st azonos sz√∂vegekhez.

#### Implement√°ci√≥

**F√°jl**: `app/nodes/retrieval_node.py`

```python
async def _get_embedding(self, text: str) -> str:
    """
    Get embedding for text with caching.
    
    In production: OpenAI embeddings API vagy lok√°lis model.
    Demo: determinisztikus hash szimul√°lja az embeddinget.
    """
    # 1. Generate cache key
    cache_key = generate_cache_key(self.CACHE_NAME, text)
    
    # 2. Check cache
    cached_embedding = await self.embedding_cache.get(cache_key)
    
    if cached_embedding is not None:
        # 3. Cache HIT - return cached embedding
        logger.info(f"Embedding cache hit")
        metrics.record_cache_lookup(
            self.CACHE_NAME,
            self.NODE_NAME,
            hit=True,
            latency=cache_lookup_time
        )
        return cached_embedding
    
    # 4. Cache MISS - compute embedding
    logger.info(f"Embedding cache miss")
    
    # In production, this would be:
    # embedding = await openai.embeddings.create(
    #     model="text-embedding-ada-002",
    #     input=text
    # )
    
    # Demo: simulate with hash
    embedding = hashlib.sha256(text.encode()).hexdigest()
    
    # 5. Save to cache
    await self.embedding_cache.set(cache_key, embedding)
    
    return embedding
```

#### Production Implement√°ci√≥ OpenAI-val

```python
import openai

async def _get_embedding_production(self, text: str) -> list[float]:
    """Production embedding with OpenAI."""
    cache_key = generate_cache_key("embedding", text)
    
    # Check cache
    cached = await self.embedding_cache.get(cache_key)
    if cached is not None:
        # Deserialize from JSON
        return json.loads(cached)
    
    # Call OpenAI
    response = await openai.embeddings.create(
        model="text-embedding-ada-002",
        input=text
    )
    
    embedding = response.data[0].embedding  # List[float]
    
    # Serialize and cache
    await self.embedding_cache.set(
        cache_key, 
        json.dumps(embedding)
    )
    
    return embedding
```

#### Mikor haszn√°ld?

‚úÖ **J√≥ esetek**:
- RAG rendszerek (document embeddings)
- Semantic search
- Similarity matching
- Ugyanazokat a dokumentumokat gyakran embeddelni kell
- K√∂lts√©ges embedding API (OpenAI: $0.0001/1K tokens)

‚ùå **Rossz esetek**:
- Egyszer haszn√°lt sz√∂vegek
- Nagyon r√∂vid sz√∂vegek (cache overhead > k√∂lts√©g)
- V√°ltoz√≥ sz√∂vegek

#### K√∂lts√©g Megtakar√≠t√°s

**OpenAI text-embedding-ada-002 pricing**: $0.0001 per 1K tokens

| Sz√∂veg hossz | Embedding k√∂lts√©g | Cache k√∂lts√©g | Megtakar√≠t√°s |
|--------------|-------------------|---------------|--------------|
| 100 tokens | $0.00001 | $0.00 | **100%** |
| 1000 tokens | $0.0001 | $0.00 | **100%** |
| 10000 tokens | $0.001 | $0.00 | **100%** |

**P√©lda**: 1000 dokumentum, 500 token √°tlag, 10x lek√©rdezve
- Cache n√©lk√ºl: 1000 √ó 10 √ó $0.00005 = **$0.50**
- Cache-el: 1000 √ó $0.00005 = **$0.05** (90% megtakar√≠t√°s)

---

### 6.3 Graph-szint≈± Cache (LangGraph Checkpointing)

#### Mi az?

A graph-szint≈± cache **a teljes workflow √°llapot√°t** menti minden node ut√°n. Ez lehet≈ëv√© teszi:
- Workflow √∫jraind√≠t√°s√°t megszak√≠t√°s ut√°n
- Human-in-the-loop pattern-ek
- Time-travel debugging
- Teljes conversation history

#### Koncepcion√°lis Implement√°ci√≥

**Jelenleg NEM implement√°lva, de √≠gy n√©zne ki:**

```python
from langgraph.checkpoint import MemorySaver, SqliteSaver

# In-memory checkpointing (development)
checkpointer = MemorySaver()

# Persistent checkpointing (production)
checkpointer = SqliteSaver.from_conn_string("checkpoints.db")

# Compile graph with checkpointing
app = workflow.compile(checkpointer=checkpointer)

# Run with thread_id for state persistence
result = await app.ainvoke(
    {"user_input": "What is Docker?"},
    config={"configurable": {"thread_id": "conversation-123"}}
)

# Continue conversation from same thread
result2 = await app.ainvoke(
    {"user_input": "Tell me more"},
    config={"configurable": {"thread_id": "conversation-123"}}
)
```

#### Graph Cache El≈ënyei

‚úÖ **El≈ëny√∂k**:
- Conversation history automatikusan t√°rolva
- Retry mechanizmus (ha node elsz√°ll)
- Human approval steps (pl. expensive m≈±velet el≈ëtt)
- Debugging: l√©p√©senk√©nt visszaj√°tszhat√≥
- Multi-turn conversations

‚ùå **H√°tr√°nyok**:
- Nagyobb mem√≥ria/disk haszn√°lat
- Komplexebb setup
- State serializ√°l√°si overhead
- Nem mindig sz√ºks√©ges

#### Mikor haszn√°ld?

‚úÖ **Haszn√°ld, ha**:
- Multi-turn conversation
- Human-in-the-loop sz√ºks√©ges
- Long-running workflows (√≥r√°k/napok)
- Retry/recovery fontos
- Audit trail kell

‚ùå **NE haszn√°ld, ha**:
- Stateless, single-turn requests
- Egyszer≈± API calls
- Nincs sz√ºks√©g history-ra
- Performance kritikus (cache overhead)

#### Implement√°ci√≥s P√©lda

**app/graph/agent_graph.py** (kiterjesztett v√°ltozat):

```python
from langgraph.checkpoint import MemorySaver

def create_graph_with_checkpointing():
    """
    Create agent graph with state persistence.
    
    This enables:
    - Conversation history
    - Human-in-the-loop
    - Workflow recovery
    """
    # Build workflow
    workflow = StateGraph(AgentState)
    
    # Add nodes...
    workflow.add_node("triage", triage_node.execute)
    workflow.add_node("retrieval", retrieval_node.execute)
    # etc...
    
    # Add edges...
    
    # IMPORTANT: Compile with checkpointer
    checkpointer = MemorySaver()  # Or SqliteSaver for persistence
    
    app = workflow.compile(checkpointer=checkpointer)
    
    return app

# Usage
app = create_graph_with_checkpointing()

# First message in conversation
result1 = await app.ainvoke(
    {"user_input": "What is Docker?"},
    config={"configurable": {"thread_id": "user-123"}}
)

# Follow-up question - has context from previous
result2 = await app.ainvoke(
    {"user_input": "How do I install it?"},
    config={"configurable": {"thread_id": "user-123"}}
)
# Graph automatically loads previous state!
```

---

### 6.4 Cache √ñsszehasonl√≠t√°s

| Cache T√≠pus | Granularit√°s | TTL | Haszn√°lat | Komplexit√°s |
|-------------|--------------|-----|-----------|-------------|
| **Node Cache** | Node eredm√©ny | 1-24 √≥ra | Ism√©tl≈ëd≈ë lek√©rdez√©sek | Alacsony |
| **Embedding Cache** | Embedding vektor | 7-30 nap | RAG document store | K√∂zepes |
| **Graph Cache** | Teljes workflow | Session | Multi-turn chat | Magas |

### 6.5 Cache Strat√©gia V√°laszt√°s

#### D√∂nt√©si Fa

```
Van ism√©tl≈ëd≈ë input?
‚îú‚îÄ Igen ‚Üí Node cache
‚îî‚îÄ Nem ‚Üí Nincs cache

Van embedding sz√°m√≠t√°s?
‚îú‚îÄ Igen ‚Üí Embedding cache
‚îî‚îÄ Nem ‚Üí Nincs embedding cache

Multi-turn conversation?
‚îú‚îÄ Igen ‚Üí Graph checkpointing
‚îî‚îÄ Nem ‚Üí Nincs graph cache
```

#### Kombin√°lt Strat√©gia (Ez az app)

```python
# Initialization
node_cache = MemoryCache(ttl_seconds=3600)      # 1 √≥ra
embedding_cache = MemoryCache(ttl_seconds=86400) # 24 √≥ra
# graph_checkpointer = None  # Not needed for stateless API

# Triage node uses node cache
triage_node = TriageNode(
    cache=node_cache  # ‚Üê Node-level caching
)

# Retrieval node uses embedding cache
retrieval_node = RetrievalNode(
    embedding_cache=embedding_cache  # ‚Üê Embedding caching
)

# Graph compiled WITHOUT checkpointing (stateless)
app = workflow.compile()  # No checkpointer
```

### 6.6 Cache Metrik√°k √©s Monitoring

#### Prometheus Metrik√°k

**F√°jl**: `app/observability/metrics.py`

```python
from prometheus_client import Counter, Histogram

# Cache operation counters
cache_hit_total = Counter(
    'cache_hit_total',
    'Total cache hits',
    ['cache', 'node']
)

cache_miss_total = Counter(
    'cache_miss_total',
    'Total cache misses',
    ['cache', 'node']
)

# Cache lookup latency
cache_lookup_latency_seconds = Histogram(
    'cache_lookup_latency_seconds',
    'Cache lookup latency',
    ['cache', 'node']
)
```

#### Cache Hit Rate Sz√°m√≠t√°s (PromQL)

```promql
# Overall cache hit rate
sum(rate(cache_hit_total[5m])) / 
  (sum(rate(cache_hit_total[5m])) + sum(rate(cache_miss_total[5m])))

# Node cache hit rate
sum(rate(cache_hit_total{cache="node_cache"}[5m])) / 
  (sum(rate(cache_hit_total{cache="node_cache"}[5m])) + 
   sum(rate(cache_miss_total{cache="node_cache"}[5m])))
```

#### Grafana Dashboard Panelek

**Cache Hit Ratio**:
- Mutassa: 0-100%
- T√≠pus: Gauge
- Threshold: <30% piros, 30-60% s√°rga, >60% z√∂ld

**Cache Operations**:
- Mutassa: hits/sec, misses/sec
- T√≠pus: Time series
- Stack: Yes

### 6.7 Cache Best Practices

#### ‚úÖ TEDD

1. **Haszn√°lj TTL-t mindig**
   ```python
   cache = MemoryCache(ttl_seconds=3600)  # 1 √≥ra
   ```

2. **√Åll√≠ts be max_size-t**
   ```python
   cache = MemoryCache(max_size=1000)  # Max 1000 elem
   ```

3. **Haszn√°lj struktur√°lt cache kulcsokat**
   ```python
   key = f"{namespace}:{hash}"  # J√≥
   # key = hash  # Rossz - n√©v√ºtk√∂z√©s
   ```

4. **Monitorozd a cache hit rate-t**
   ```python
   metrics.record_cache_lookup(cache_name, node, hit=True)
   ```

5. **Dokument√°ld a TTL strat√©gi√°t**
   ```python
   # Triage: 1 √≥ra (classification stabil)
   # Embedding: 24 √≥ra (dokumentumok ritk√°n v√°ltoznak)
   ```

#### ‚ùå NE TEDD

1. **Ne cache-elj nem-determinisztikus m≈±veleteket**
   ```python
   # ROSSZ: Creative writing random eredm√©nyt ad
   cache.set("creative_story", random_story)
   ```

2. **Ne haszn√°lj t√∫l hossz√∫ TTL-t**
   ```python
   # ROSSZ: 30 nap TTL r√©gi adatokhoz vezet
   cache = MemoryCache(ttl_seconds=30*24*3600)
   ```

3. **Ne felejtsd el a cache eviction-t**
   ```python
   # ROSSZ: V√©gtelen n√∂veked√©s
   cache = MemoryCache(max_size=None)  # Mem√≥ria leak!
   ```

4. **Ne cache-elj √©rz√©keny adatokat**
   ```python
   # ROSSZ: Jelsz√≥ cache-ben
   cache.set("password", user_password)  # Biztons√°gi kock√°zat!
   ```

---

## √ñsszes√≠tett Hat√°s

### Teljes P√©lda: Egyszer≈± Lek√©rdez√©s

**Lek√©rdez√©s**: "What is 2+2?"

#### ‚ùå Rossz Verzi√≥ V√©grehajt√°s

```
1. TRIAGE NODE
   - Model: GPT-4 ($0.01/$0.03)
   - Prompt: 350 tokens
   - Max tokens: 2000
   - Output: ~5 tokens ("simple")
   - K√∂lts√©g: (350 √ó 0.01 + 5 √ó 0.03) / 1000 = $0.0035 + $0.00015 = $0.00365
   - Cache: Nincs

2. RETRIEVAL NODE (felesleges!)
   - Model: GPT-4
   - Embedding compute + lookup
   - K√∂lts√©g: ~$0.004
   - Cache: Nincs

3. REASONING NODE (felesleges!)
   - Model: GPT-4
   - Prompt: 420 tokens
   - Max tokens: 3000
   - Output: ~800 tokens
   - K√∂lts√©g: (420 √ó 0.01 + 800 √ó 0.03) / 1000 = $0.0042 + $0.024 = $0.0282
   - Cache: Nincs

4. SUMMARY NODE
   - Model: GPT-4
   - Prompt: 380 tokens
   - Max tokens: 2000
   - Output: ~150 tokens
   - K√∂lts√©g: (380 √ó 0.01 + 150 √ó 0.03) / 1000 = $0.0038 + $0.0045 = $0.0083

√ñSSZ K√ñLTS√âG: $0.00365 + $0.004 + $0.0282 + $0.0083 = $0.04415
LATENCY: ~5 seconds
NODES: 4
```

#### ‚úÖ J√≥ Verzi√≥ V√©grehajt√°s (els≈ë fut√°s)

```
1. TRIAGE NODE
   - Model: GPT-3.5-turbo ($0.0001/$0.0002)
   - Prompt: 45 tokens
   - Max tokens: 10
   - Output: 1 token ("simple")
   - K√∂lts√©g: (45 √ó 0.0001 + 1 √ó 0.0002) / 1000 = $0.0000045 + $0.0000002 = $0.0000047
   - Cache: Miss ‚Üí ment√©s

2. RETRIEVAL NODE
   - SKIPPED (routing: simple ‚Üí summary)

3. REASONING NODE
   - SKIPPED (routing: simple ‚Üí summary)

4. SUMMARY NODE
   - Model: GPT-4-turbo ($0.001/$0.002)
   - Prompt: 30 tokens
   - Max tokens: 500
   - Output: ~20 tokens
   - K√∂lts√©g: (30 √ó 0.001 + 20 √ó 0.002) / 1000 = $0.00003 + $0.00004 = $0.00007

√ñSSZ K√ñLTS√âG: $0.0000047 + $0.00007 = $0.0000747
LATENCY: ~1.2 seconds
NODES: 2
```

#### ‚úÖ J√≥ Verzi√≥ V√©grehajt√°s (m√°sodik fut√°s - cache hit)

```
1. TRIAGE NODE
   - Cache HIT! ‚Üí "simple"
   - LLM h√≠v√°s: NINCS
   - K√∂lts√©g: $0.00000
   - Latency: ~5ms

2. RETRIEVAL NODE
   - SKIPPED

3. REASONING NODE
   - SKIPPED

4. SUMMARY NODE
   - Model: GPT-4-turbo
   - K√∂lts√©g: ~$0.00007

√ñSSZ K√ñLTS√âG: $0.00007
LATENCY: ~0.5 seconds
NODES: 2 (1 cached)
```

### √ñsszehasonl√≠t√°s

| Metrika | Rossz | J√≥ (1. fut√°s) | J√≥ (2. fut√°s) |
|---------|-------|---------------|---------------|
| **K√∂lts√©g** | $0.044 | $0.000075 | $0.00007 |
| **Latency** | 5s | 1.2s | 0.5s |
| **LLM h√≠v√°sok** | 4 | 2 | 1 |
| **Node-ok** | 4 | 2 | 2 |
| **Cache haszn√°lat** | 0% | 50% | 50% |

**Megtakar√≠t√°s**:
- Els≈ë fut√°s: **99.8%** k√∂lts√©g cs√∂kken√©s
- M√°sodik fut√°s: **99.84%** k√∂lts√©g cs√∂kken√©s
- Latency javul√°s: **76-90%**

### Sk√°l√°zhat√≥s√°gi Hat√°s

**Havi 100,000 lek√©rdez√©s** (50% egyszer≈±, 30% retrieval, 20% komplex):

| Verzi√≥ | Havi K√∂lts√©g | √âves K√∂lts√©g |
|--------|--------------|--------------|
| Rossz | **$2,200** | **$26,400** |
| J√≥ (cache n√©lk√ºl) | **$150** | **$1,800** |
| J√≥ (40% cache hit) | **$90** | **$1,080** |

**Megtakar√≠t√°s**: **$25,320/√©v** (96% cs√∂kken√©s)

---

## Implement√°l√°si Checklist Hallgat√≥knak

### 1. Prompt Optimaliz√°l√°s ‚úÖ

- [ ] T√∂r√∂ld az √∂sszes felesleges bevezet≈ët √©s magyar√°zatot
- [ ] Haszn√°lj r√∂vid, utas√≠t√°sszer≈± nyelvezetet
- [ ] Csak a sz√ºks√©ges inform√°ci√≥kat add meg
- [ ] Teszteld: minimum 80% token cs√∂kken√©s

**F√°jlok**:
- `prompts/triage_prompt.txt`
- `prompts/reasoning_prompt.txt`
- `prompts/summary_prompt.txt`

### 2. Modell V√°laszt√°s ‚úÖ

- [ ] Triage node: `ModelTier.CHEAP`
- [ ] Retrieval node: `ModelTier.CHEAP`
- [ ] Summary node: `ModelTier.MEDIUM`
- [ ] Reasoning node: `ModelTier.EXPENSIVE` (csak ha sz√ºks√©ges)

**F√°jlok**:
- `app/nodes/triage_node.py` - `__init__` met√≥dus
- `app/nodes/retrieval_node.py` - `__init__` met√≥dus
- `app/nodes/summary_node.py` - `__init__` met√≥dus

### 3. Cache Enged√©lyez√©se ‚úÖ

- [ ] Triage node: `cached_result = await self.cache.get(cache_key)`
- [ ] Triage node: `await self.cache.set(cache_key, classification)`
- [ ] Retrieval node: `cached_embedding = await self.embedding_cache.get(cache_key)`
- [ ] Retrieval node: `await self.embedding_cache.set(cache_key, embedding)`

**F√°jlok**:
- `app/nodes/triage_node.py` - `execute` met√≥dus
- `app/nodes/retrieval_node.py` - `_get_embedding` met√≥dus

### 4. Conditional Routing ‚úÖ

- [ ] Implement√°ld `route_after_triage` intelligens logik√°val
- [ ] Implement√°ld `route_after_retrieval` intelligens logik√°val
- [ ] Add hozz√° early exit logik√°t a reasoning node-hoz
- [ ] Add hozz√° early exit logik√°t a retrieval node-hoz

**F√°jlok**:
- `app/graph/agent_graph.py` - routing f√ºggv√©nyek
- `app/nodes/reasoning_node.py` - `execute` met√≥dus elej√©n
- `app/nodes/retrieval_node.py` - `execute` met√≥dus elej√©n

### 5. Token Limitek ‚úÖ

- [ ] Triage: `max_tokens=10`
- [ ] Reasoning: `max_tokens=1000`
- [ ] Summary: `max_tokens=500`

**F√°jlok**:
- `app/nodes/triage_node.py` - `execute` met√≥dus, `llm_client.complete` h√≠v√°s
- `app/nodes/reasoning_node.py` - `execute` met√≥dus, `llm_client.complete` h√≠v√°s
- `app/nodes/summary_node.py` - `execute` met√≥dus, `llm_client.complete` h√≠v√°s

---

## Tesztel√©s

### Lok√°lis Teszt

```bash
# Ind√≠tsd el a szolg√°ltat√°sokat
docker compose up --build

# Teszt: egyszer≈± lek√©rdez√©s
curl -X POST http://localhost:8000/run \
  -H "Content-Type: application/json" \
  -d '{"user_input": "What is 2+2?"}'

# Elv√°rt eredm√©ny:
# - nodes_executed: ["triage", "summary"]
# - cache_hits: {triage: false} (els≈ë fut√°s)
# - models_used: ["gpt-3.5-turbo", "gpt-4-turbo"]
# - total_cost_usd: ~$0.00008
```

### Cache Teszt

```bash
# Ugyanaz a lek√©rdez√©s 3x
for i in {1..3}; do
  curl -X POST http://localhost:8000/run \
    -H "Content-Type: application/json" \
    -d '{"user_input": "What is Docker?"}'
  echo ""
  sleep 2
done

# Elv√°rt:
# 1. fut√°s: cache_hits: {triage: false}
# 2. fut√°s: cache_hits: {triage: true}  ‚Üê FONTOS!
# 3. fut√°s: cache_hits: {triage: true}
```

### Grafana Metrik√°k

Nyisd meg: http://localhost:3000

Ellen≈ërizd:
- ‚úÖ `llm_inference_count_total{model="gpt-3.5-turbo"}` - n≈ë (triage)
- ‚úÖ `llm_inference_count_total{model="gpt-4-turbo"}` - n≈ë (summary)
- ‚úÖ `llm_inference_count_total{model="gpt-4"}` - NEM n≈ë (egyszer≈± lek√©rdez√©sekn√©l)
- ‚úÖ `cache_hit_total{cache="node_cache"}` - n≈ë a m√°sodik fut√°st√≥l
- ‚úÖ `llm_cost_total_usd` - alacsony marad

---

## Gyakori Hib√°k √©s Megold√°sok

### Hiba 1: Cache nem m≈±k√∂dik

**T√ºnet**: `cache_hits` mindig `false`

**Ok**: Elfelejtett√©l `await` kulcssz√≥t haszn√°lni

```python
# ‚ùå ROSSZ
cached_result = self.cache.get(cache_key)  # Nem await!

# ‚úÖ J√ì
cached_result = await self.cache.get(cache_key)
```

### Hiba 2: Minden node mindig fut

**T√ºnet**: `nodes_executed` mindig 4 node

**Ok**: Nem implement√°ltad a conditional routing-ot

**Megold√°s**: Ellen≈ërizd `app/graph/agent_graph.py` routing logik√°t

### Hiba 3: M√©g mindig dr√°ga

**T√ºnet**: `total_cost_usd` > $0.01 egyszer≈± lek√©rdez√©sn√©l

**Ok**: 
1. Nem cser√©lt a cheap model-re a triage
2. Nem cs√∂kkentetted a max_tokens-t
3. Verbose prompts haszn√°lata

**Megold√°s**: Ellen≈ërizd mind az 5 jav√≠t√°si pontot

### Hiba 4: SyntaxError a promptokban

**T√ºnet**: Prompt f√°jl bet√∂lt√©si hiba

**Ok**: Elfelejtett `"""` a docstring-ben

**Megold√°s**: Ellen≈ërizd az id√©z≈ëjeleket:
```python
def _build_prompt(self, state: AgentState) -> str:
    """
    Build prompt.
    """  # ‚Üê Fontos: 3 id√©z≈ëjel
    # ...
```

---

## K√∂vetkez≈ë L√©p√©sek

1. ‚úÖ Implement√°ld mind az 5 jav√≠t√°st
2. ‚úÖ Teszteld lok√°lisan
3. ‚úÖ Ellen≈ërizd a Grafana metrik√°kat
4. ‚úÖ Dokument√°ld a v√°ltoztat√°sokat
5. ‚úÖ Commit-old a k√≥dot git-be

**Sikeres implement√°ci√≥ jele**:
- 90%+ k√∂lts√©g cs√∂kken√©s
- 40%+ cache hit ratio (m√°sodik fut√°st√≥l)
- 2-3 node √°tlagosan (nem mindig 4)
- Sub-second latency cache hit eset√©n

---

## üéØ Gyakorl√≥ Feladatok Hallgat√≥knak

Ezek a feladatok tov√°bbi k√∂lts√©goptimaliz√°l√°si technik√°kat vezetnek be, amelyek tov√°bb jav√≠tj√°k az alkalmaz√°s hat√©konys√°g√°t √©s k√∂lts√©ghat√©konys√°g√°t.

---

### Feladat 1: Streaming Response Implement√°l√°s

**Neh√©zs√©g**: ‚≠ê‚≠ê‚≠ê (K√∂zepes)

**C√©l**: Implement√°lj streaming v√°laszt a Summary node-ban, hogy a felhaszn√°l√≥ hamarabb l√°thasson r√©szleges eredm√©nyeket.

**Mit kell csin√°lni:**

1. M√≥dos√≠tsd az `OpenAIClient` oszt√°lyt `app/llm/openai_client.py`-ben
2. Adj hozz√° `stream=True` param√©tert a `complete()` met√≥dushoz
3. Implement√°lj `stream_complete()` met√≥dust, ami yield-eli a tokeneket
4. M√≥dos√≠tsd `app/nodes/summary_node.py`-t, hogy haszn√°lja a streaming-et
5. Friss√≠tsd a FastAPI endpoint-ot `app/main.py`-ben `StreamingResponse`-ra

**K√≥d v√°zlat:**

```python
# app/llm/openai_client.py
async def stream_complete(
    self, 
    messages: List[Dict], 
    model_name: str,
    max_tokens: int = 1000
) -> AsyncGenerator[str, None]:
    """Stream LLM response token-by-token."""
    response = await self.client.chat.completions.create(
        model=model_name,
        messages=messages,
        max_tokens=max_tokens,
        stream=True  # ‚Üê Fontos!
    )
    
    async for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content
```

**M√©rt hat√°s:**
- ‚è±Ô∏è √âszlelt latency: -60% (user hamarabb l√°t eredm√©nyt)
- üí∞ K√∂lts√©g: v√°ltozatlan
- üìä UX: jelent≈ësen jobb

**Ellen≈ërz√©s:**
```bash
curl -N http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{"user_input": "What is Docker?"}' \
| jq -r '.response'
# L√°tni kell, ahogy a v√°lasz fokozatosan √©rkezik
```

---

### Feladat 2: Response Caching (Teljes Node Cache)

**Neh√©zs√©g**: ‚≠ê‚≠ê (K√∂nny≈±)

**C√©l**: Implement√°lj teljes v√°lasz cache-t a Summary node-ra, hogy ugyanazokra a k√©rd√©sekre azonnali v√°laszt adjon.

**Mit kell csin√°lni:**

1. Adj hozz√° `cache` param√©tert a `SummaryNode.__init__()`-hez
2. Gener√°lj cache key-t az `user_input` + `classification` alapj√°n
3. Cache-eld a teljes `final_response`-t
4. Ellen≈ërizd a cache-t a Summary node fut√°sa el≈ëtt
5. √Åll√≠tsd be a TTL-t 24 √≥r√°ra (stabil v√°laszok eset√©n)

**K√≥d v√°zlat:**

```python
# app/nodes/summary_node.py
async def execute(self, state: AgentState) -> Dict:
    """Execute summary with response caching."""
    
    # Generate cache key
    cache_content = f"{state['user_input']}:{state.get('classification', '')}"
    cache_key = generate_cache_key("summary_response", cache_content)
    
    # Check cache
    cached_response = await self.cache.get(cache_key)
    if cached_response is not None:
        logger.info("Summary cache HIT - returning cached response")
        return {
            "final_response": cached_response,
            "cache_hit": True
        }
    
    # Cache MISS - generate response
    # ... (megl√©v≈ë k√≥d) ...
    
    # Save to cache
    await self.cache.set(cache_key, final_response)
    
    return {"final_response": final_response, "cache_hit": False}
```

**Konfigur√°l√°s:**

```python
# app/config.py
class Settings(BaseSettings):
    summary_cache_ttl_seconds: int = 86400  # 24 √≥ra
```

**M√©rt hat√°s:**
- üí∞ K√∂lts√©g: -100% (cache hit eset√©n)
- ‚è±Ô∏è Latency: -95% (6s ‚Üí 0.3s)
- üìä Cache hit ratio: 20-40% (FAQ t√≠pus√∫ k√©rd√©sekn√©l)

**Ellen≈ërz√©s:**
```bash
# Els≈ë h√≠v√°s - cache miss
time curl -X POST http://localhost:8000/query -d '{"user_input":"What is Docker?"}' -H "Content-Type: application/json"
# ~4-6s

# M√°sodik h√≠v√°s - cache hit
time curl -X POST http://localhost:8000/query -d '{"user_input":"What is Docker?"}' -H "Content-Type: application/json"
# ~0.3s
```

---

### Feladat 3: Batch Query T√°mogat√°s

**Neh√©zs√©g**: ‚≠ê‚≠ê‚≠ê‚≠ê (Neh√©z)

**C√©l**: Implement√°lj batch processing-et, ahol t√∂bb k√©rd√©st egyszerre lehet feldolgozni, √©s az OpenAI batch API-t haszn√°lva olcs√≥bban.

**Mit kell csin√°lni:**

1. Hozz l√©tre √∫j endpoint-ot: `POST /batch-query`
2. Fogadj JSON array-t k√©rd√©sekkel: `{"queries": ["q1", "q2", "q3"]}`
3. Implement√°lj batch feldolgoz√°st `asyncio.gather()`-rel
4. Haszn√°ld az OpenAI Batch API-t (50% olcs√≥bb, de 24h k√©sleltet√©ssel)
5. T√°rold a batch job ID-kat Redis-ben vagy f√°jlban
6. Adj hozz√° `GET /batch-status/{job_id}` endpoint-ot

**K√≥d v√°zlat:**

```python
# app/main.py
@app.post("/batch-query")
async def batch_query(request: BatchQueryRequest):
    """Process multiple queries in batch mode."""
    
    # Option 1: Aszinkron p√°rhuzamos feldolgoz√°s (azonnal)
    tasks = [
        process_query(query) 
        for query in request.queries
    ]
    results = await asyncio.gather(*tasks)
    
    return {"results": results}

# Option 2: OpenAI Batch API (24h, 50% olcs√≥bb)
@app.post("/batch-query-async")
async def batch_query_async(request: BatchQueryRequest):
    """Submit batch job to OpenAI Batch API."""
    
    # Prepare batch file
    batch_requests = [
        {
            "custom_id": f"request-{i}",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "gpt-3.5-turbo",
                "messages": [{"role": "user", "content": q}]
            }
        }
        for i, q in enumerate(request.queries)
    ]
    
    # Submit to OpenAI Batch API
    batch = await openai.batches.create(
        input_file_id=uploaded_file_id,
        endpoint="/v1/chat/completions",
        completion_window="24h"
    )
    
    return {
        "batch_id": batch.id,
        "status": "processing",
        "estimated_completion": "24 hours"
    }
```

**M√©rt hat√°s:**
- üí∞ K√∂lts√©g: -50% (Batch API haszn√°lat√°val)
- ‚è±Ô∏è Throughput: 5-10x (p√°rhuzamos feldolgoz√°s)
- üìä Komplexit√°s: +40%

**Ellen≈ërz√©s:**
```bash
# Batch query (parallel)
curl -X POST http://localhost:8000/batch-query \
  -H "Content-Type: application/json" \
  -d '{
    "queries": [
      "What is Docker?",
      "What is Kubernetes?",
      "What is CI/CD?"
    ]
  }'
```

---

### Feladat 4: Token Usage Limit & Rate Limiting

**Neh√©zs√©g**: ‚≠ê‚≠ê (K√∂nny≈±-K√∂zepes)

**C√©l**: Implement√°lj token quota rendszert, ami meg√°ll√≠tja a feldolgoz√°st, ha a felhaszn√°l√≥ t√∫ll√©pi a napi limitet.

**Mit kell csin√°lni:**

1. Hozz l√©tre `TokenQuotaTracker` oszt√°lyt
2. T√°rold a felhaszn√°l√≥nk√©nti token haszn√°latot in-memory vagy Redis-ben
3. Ellen≈ërizd a quota-t minden request el≈ëtt
4. Add vissza `429 Too Many Requests` hib√°t, ha t√∫ll√©p√©s van
5. Adj hozz√° `/quota` endpoint-ot a fennmarad√≥ quota ellen≈ërz√©s√©re

**K√≥d v√°zlat:**

```python
# app/utils/quota.py
class TokenQuotaTracker:
    """Track per-user token usage with daily limits."""
    
    def __init__(self, daily_limit: int = 100000):
        self._usage = {}  # {user_id: {date: token_count}}
        self._daily_limit = daily_limit
    
    def check_quota(self, user_id: str) -> bool:
        """Check if user has remaining quota."""
        today = datetime.now().date()
        usage_today = self._usage.get(user_id, {}).get(today, 0)
        return usage_today < self._daily_limit
    
    def add_usage(self, user_id: str, tokens: int):
        """Add token usage for user."""
        today = datetime.now().date()
        if user_id not in self._usage:
            self._usage[user_id] = {}
        self._usage[user_id][today] = (
            self._usage[user_id].get(today, 0) + tokens
        )
    
    def get_remaining(self, user_id: str) -> int:
        """Get remaining quota."""
        today = datetime.now().date()
        used = self._usage.get(user_id, {}).get(today, 0)
        return max(0, self._daily_limit - used)

# app/main.py
quota_tracker = TokenQuotaTracker(daily_limit=100000)

@app.post("/query")
async def query(request: QueryRequest, user_id: str = "default"):
    # Check quota
    if not quota_tracker.check_quota(user_id):
        raise HTTPException(
            status_code=429,
            detail=f"Daily token quota exceeded. Try again tomorrow."
        )
    
    # Process query
    result = await agent.run(request.user_input)
    
    # Track usage
    quota_tracker.add_usage(user_id, result["total_tokens"])
    
    return result

@app.get("/quota")
async def get_quota(user_id: str = "default"):
    """Check remaining quota."""
    return {
        "remaining_tokens": quota_tracker.get_remaining(user_id),
        "daily_limit": quota_tracker._daily_limit
    }
```

**Konfigur√°l√°s:**

```python
# app/config.py
class Settings(BaseSettings):
    token_quota_daily: int = 100000  # 100K tokens/day
    token_quota_enabled: bool = True
```

**M√©rt hat√°s:**
- üí∞ K√∂lts√©g v√©delem: Megakad√°lyozza a v√°ratlan k√∂lts√©geket
- üõ°Ô∏è Rate limiting: V√©delem abuse ellen
- üìä Fair use: Egyenletes terhel√©s

**Ellen≈ërz√©s:**
```bash
# Check quota
curl http://localhost:8000/quota?user_id=test_user
# {"remaining_tokens": 95000, "daily_limit": 100000}

# Exhaust quota (loop)
for i in {1..100}; do
  curl -X POST http://localhost:8000/query \
    -H "Content-Type: application/json" \
    -d '{"user_input":"Test"}' \
    -H "X-User-ID: test_user"
done

# Should eventually get 429 error
```

---

### Feladat 5: Semantic Cache (Vector-based Caching)

**Neh√©zs√©g**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Halad√≥)

**C√©l**: Implement√°lj szemantikus cache-t, ami hasonl√≥ k√©rd√©seket is felismer (nem csak exact match).

**Koncepci√≥:**
A hagyom√°nyos cache csak akkor tal√°l, ha **pontosan ugyanaz** a k√©rd√©s. A semantic cache **hasonl√≥ jelent√©s≈±** k√©rd√©sekre is cache-b≈ël v√°laszol.

**P√©lda:**
```
Query 1: "What is Docker?"
Query 2: "Can you explain Docker to me?"
Query 3: "Tell me about Docker"

‚Üí Hagyom√°nyos cache: 3 cache miss
‚Üí Semantic cache: 1 cache miss, 2 cache hit (hasonl√≥s√°g alapj√°n)
```

**Mit kell csin√°lni:**

1. Telep√≠tsd a `faiss-cpu` vagy `chromadb` library-t
2. Hozz l√©tre `SemanticCache` oszt√°lyt
3. Minden k√©rd√©st embeddelj (OpenAI `text-embedding-ada-002`)
4. T√°rold az embedding + v√°lasz p√°rokat vector DB-ben
5. Keres√©s: keresd a legk√∂zelebbi embeddinget (cosine similarity)
6. Ha similarity > 0.95, add vissza a cached v√°laszt

**K√≥d v√°zlat:**

```python
# app/cache/semantic_cache.py
import numpy as np
from typing import Optional, List
import openai

class SemanticCache:
    """Vector-based cache for semantic similarity matching."""
    
    def __init__(self, similarity_threshold: float = 0.95):
        self._embeddings = []  # List of embedding vectors
        self._responses = []   # Corresponding responses
        self._queries = []     # Original queries (for debugging)
        self._threshold = similarity_threshold
    
    async def _get_embedding(self, text: str) -> np.ndarray:
        """Get embedding vector from OpenAI."""
        response = await openai.embeddings.create(
            model="text-embedding-ada-002",
            input=text
        )
        return np.array(response.data[0].embedding)
    
    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors."""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    async def get(self, query: str) -> Optional[str]:
        """Find similar cached response."""
        if not self._embeddings:
            return None
        
        # Get query embedding
        query_emb = await self._get_embedding(query)
        
        # Find most similar
        max_similarity = 0.0
        best_idx = -1
        
        for i, cached_emb in enumerate(self._embeddings):
            similarity = self._cosine_similarity(query_emb, cached_emb)
            if similarity > max_similarity:
                max_similarity = similarity
                best_idx = i
        
        # Check threshold
        if max_similarity >= self._threshold:
            logger.info(
                f"Semantic cache HIT: '{query}' ‚âà '{self._queries[best_idx]}' "
                f"(similarity: {max_similarity:.3f})"
            )
            return self._responses[best_idx]
        
        logger.info(f"Semantic cache MISS (best: {max_similarity:.3f})")
        return None
    
    async def set(self, query: str, response: str):
        """Store query-response pair with embedding."""
        embedding = await self._get_embedding(query)
        self._embeddings.append(embedding)
        self._responses.append(response)
        self._queries.append(query)
```

**Haszn√°lat:**

```python
# app/nodes/summary_node.py
from app.cache.semantic_cache import SemanticCache

class SummaryNode:
    def __init__(self, semantic_cache: SemanticCache):
        self.semantic_cache = semantic_cache
    
    async def execute(self, state: AgentState) -> Dict:
        # Check semantic cache
        cached = await self.semantic_cache.get(state["user_input"])
        if cached is not None:
            return {"final_response": cached, "semantic_cache_hit": True}
        
        # Generate response
        response = await self._generate_response(state)
        
        # Store in semantic cache
        await self.semantic_cache.set(state["user_input"], response)
        
        return {"final_response": response}
```

**K√∂lts√©g-Haszon Elemz√©s:**

| M≈±velet | K√∂lts√©g | Hat√°s |
|---------|---------|-------|
| Embedding gener√°l√°s | $0.0001/1K tok | +$0.00001 per query |
| Semantic search | CPU only | Negligible |
| Cache hit saving | $0.0015 | **150x megt√©r√ºl√©s** |

**Akkor √©ri meg, ha**: Cache hit rate > 0.5% (azaz 200 k√©rd√©sb≈ël 1 hasonl√≥)

**M√©rt hat√°s:**
- üí∞ K√∂lts√©g: +1% (embedding), -20% (cache hits)
- üéØ Cache hit rate: +15-25% (hasonl√≥ k√©rd√©sekn√©l)
- üìä UX: Konzisztens v√°laszok hasonl√≥ k√©rd√©sekre

**Ellen≈ërz√©s:**
```python
# Test semantic similarity
queries = [
    "What is Docker?",
    "Can you explain Docker?",
    "Tell me about Docker",
    "Docker nedir?",  # Turkish - should be similar
    "What is Kubernetes?",  # Different - should NOT match
]

for query in queries:
    result = await semantic_cache.get(query)
    print(f"{query}: {'HIT' if result else 'MISS'}")

# Expected:
# What is Docker?: MISS (first)
# Can you explain Docker?: HIT (similarity ~0.96)
# Tell me about Docker: HIT (similarity ~0.97)
# Docker nedir?: HIT (similarity ~0.92)
# What is Kubernetes?: MISS (similarity ~0.75)
```

---

### Feladat 6: Model Fallback Strategy

**Neh√©zs√©g**: ‚≠ê‚≠ê‚≠ê (K√∂zepes)

**C√©l**: Implement√°lj automatikus fallback strat√©gi√°t, ami olcs√≥bb modellre v√°lt, ha a dr√°g√°bb modell hib√°zik vagy t√∫l lass√∫.

**Mit kell csin√°lni:**

1. M√≥dos√≠tsd az `OpenAIClient`-et, hogy t√°mogassa a model tier fallback-et
2. Ha GPT-4 rate limit-et kap, pr√≥b√°lja meg GPT-3.5-tel
3. Ha timeout t√∂rt√©nik, pr√≥b√°ld √∫jra r√∂videbb kontextussal
4. Logold a fallback esem√©nyeket Prometheus-ba

**K√≥d v√°zlat:**

```python
# app/llm/openai_client.py
class OpenAIClient:
    async def complete_with_fallback(
        self,
        messages: List[Dict],
        model_tier: ModelTier,
        max_tokens: int = 1000,
        timeout: int = 30
    ) -> CompletionResponse:
        """Complete with automatic fallback on failure."""
        
        # Try primary model
        primary_model = self.model_selector.get_model_name(model_tier)
        
        try:
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=primary_model,
                    messages=messages,
                    max_tokens=max_tokens
                ),
                timeout=timeout
            )
            return CompletionResponse.from_openai(response)
            
        except asyncio.TimeoutError:
            logger.warning(f"{primary_model} timeout, falling back...")
            metrics.fallback_total.labels(
                reason="timeout",
                from_model=primary_model
            ).inc()
            
            # Fallback: use cheaper model
            fallback_tier = self._get_fallback_tier(model_tier)
            fallback_model = self.model_selector.get_model_name(fallback_tier)
            
            response = await self.client.chat.completions.create(
                model=fallback_model,
                messages=messages,
                max_tokens=max_tokens
            )
            return CompletionResponse.from_openai(response)
            
        except openai.RateLimitError:
            logger.warning(f"{primary_model} rate limited, falling back...")
            metrics.fallback_total.labels(
                reason="rate_limit",
                from_model=primary_model
            ).inc()
            
            # Fallback to cheaper model
            fallback_tier = self._get_fallback_tier(model_tier)
            fallback_model = self.model_selector.get_model_name(fallback_tier)
            
            response = await self.client.chat.completions.create(
                model=fallback_model,
                messages=messages,
                max_tokens=max_tokens
            )
            return CompletionResponse.from_openai(response)
    
    def _get_fallback_tier(self, tier: ModelTier) -> ModelTier:
        """Get cheaper fallback tier."""
        fallback_map = {
            ModelTier.EXPENSIVE: ModelTier.MEDIUM,
            ModelTier.MEDIUM: ModelTier.CHEAP,
            ModelTier.CHEAP: ModelTier.CHEAP,  # No fallback
        }
        return fallback_map[tier]
```

**M√©rt hat√°s:**
- üõ°Ô∏è Reliability: +30% (kevesebb hiba)
- üí∞ K√∂lts√©g: -10% (fallback olcs√≥bb)
- ‚è±Ô∏è Latency: -5% (gyorsabb fallback model)

---

## üìä Feladatok √ñsszes√≠t√©se

| Feladat | Neh√©zs√©g | K√∂lts√©g hat√°s | Teljes√≠tm√©ny hat√°s | Id≈ëig√©ny |
|---------|----------|---------------|-------------------|----------|
| **1. Streaming** | ‚≠ê‚≠ê‚≠ê | 0% | UX: +60% | 2-3 √≥ra |
| **2. Response Cache** | ‚≠ê‚≠ê | -100% (hit) | -95% latency | 1-2 √≥ra |
| **3. Batch API** | ‚≠ê‚≠ê‚≠ê‚≠ê | -50% | +500% throughput | 4-6 √≥ra |
| **4. Quota System** | ‚≠ê‚≠ê | V√©delem | Unchanged | 1-2 √≥ra |
| **5. Semantic Cache** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | -20% | +15% cache hit | 6-8 √≥ra |
| **6. Model Fallback** | ‚≠ê‚≠ê‚≠ê | -10% | +30% reliability | 2-3 √≥ra |

### Aj√°nlott Sorrend

1. **Kezd≈ë szint**: Feladat 2 (Response Cache) ‚Üí Feladat 4 (Quota)
2. **K√∂zepes szint**: Feladat 1 (Streaming) ‚Üí Feladat 6 (Fallback)
3. **Halad√≥ szint**: Feladat 3 (Batch) ‚Üí Feladat 5 (Semantic Cache)

### √ârt√©kel√©si Krit√©riumok

Minden feladathoz:
- ‚úÖ M≈±k√∂d≈ë k√≥d implement√°ci√≥
- ‚úÖ Unit tesztek (min. 80% coverage)
- ‚úÖ Prometheus metrik√°k integr√°l√°sa
- ‚úÖ README friss√≠t√©se haszn√°lati p√©ld√°kkal
- ‚úÖ Grafana dashboard panel hozz√°ad√°sa (ha relev√°ns)
- ‚úÖ Cost/benefit anal√≠zis a commit message-ben

**Plusz pontok:**
- üåü Docker environment v√°ltoz√≥k t√°mogat√°sa
- üåü Error handling √©s logging
- üåü API dokument√°ci√≥ (OpenAPI/Swagger)
- üåü Load testing eredm√©nyek (Locust/k6)

---

**K√©sz√≠tette**: AI Agent Optimization Course  
**D√°tum**: 2026. janu√°r 18.  
**Verzi√≥**: 1.1  
**Licenc**: MIT - Oktat√°si c√©lokra
